{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    return grouped_data, unique_word_list\n",
    "\n",
    "dataset, unique_word_list = read_and_preprocess_data()\n",
    "\n",
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def get_word_dims_from_ansatz(ANSATZ):\n",
    "    noun = ANSATZ.ob_map[Ty('n')]\n",
    "    sent = ANSATZ.ob_map[Ty('s')]\n",
    "    if isinstance(ANSATZ, IQPAnsatz):\n",
    "        noun_parameters = 3 if noun == 1 else (noun-1)\n",
    "        subject_parameters = noun + noun + sent - 1\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim14Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*4\n",
    "        subject_parameters = 4*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim15Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*2\n",
    "        subject_parameters = 2*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, StronglyEntanglingAnsatz):\n",
    "        print(\"ERROR NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "def get_datasets(ansatz, seed, batch_size):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=batch_size)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=batch_size)\n",
    "    return diagrams, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Reduced Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_pca(input_array, new_dims):\n",
    "    pca = PCA(n_components=new_dims)\n",
    "    pca.fit(input_array)\n",
    "    data_pca = pca.transform(input_array)\n",
    "    return data_pca\n",
    "\n",
    "def reduce_by_kpca(input_array, new_dims, kernel='rbf'):\n",
    "    kpca = KernelPCA(n_components=new_dims, kernel=kernel)\n",
    "    data_kpca = kpca.fit_transform(input_array)\n",
    "    return data_kpca\n",
    "\n",
    "def reduce_by_svd(input_array, new_dims):\n",
    "    U, D, Vt = np.linalg.svd(input_array)\n",
    "    U_reduced = U[:, :new_dims]\n",
    "    A_reduced = np.dot(U_reduced, np.diag(D[:new_dims]))\n",
    "    return A_reduced\n",
    "\n",
    "def reduce_by_mds(input_array, new_dims):\n",
    "    mds = MDS(n_components=new_dims, normalized_stress='auto')\n",
    "    data_mds = mds.fit_transform(input_array)\n",
    "    return data_mds\n",
    "\n",
    "def reduce_by_isomap(input_array, new_dims, n_neighbors=5):\n",
    "    pairwise_distances = squareform(pdist(input_array))\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=new_dims)\n",
    "    data_isomap = isomap.fit_transform(pairwise_distances)\n",
    "    return data_isomap\n",
    "\n",
    "def reduce_by_tsne(input_array, new_dims, perplexity=30, learning_rate=200):\n",
    "    tsne = TSNE(n_components=new_dims, perplexity=perplexity, learning_rate=learning_rate, method='exact')\n",
    "    data_tsne = tsne.fit_transform(input_array)\n",
    "    return data_tsne\n",
    "\n",
    "def sammon_mapping_loss(Y, X, delta):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    sum_delta = np.sum(delta)\n",
    "    d_ij = pdist(Y)\n",
    "    d_ij[d_ij == 0] = 1e-10  # Avoid division by zero\n",
    "    loss = np.sum((d_ij - delta) ** 2 / (d_ij * delta)) / (2 * sum_delta)\n",
    "    return loss\n",
    "\n",
    "def reduce_by_sammon(input_array, new_dims):\n",
    "    D = pdist(input_array)  # Calculate distance matrix using the original high-dimensional data\n",
    "    first_guess = np.random.rand(input_array.shape[0], new_dims)\n",
    "    print(first_guess.shape)\n",
    "    result = minimize(\n",
    "        lambda Y: sammon_mapping_loss(Y, input_array, D),\n",
    "        first_guess,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    data_sammon = result.x\n",
    "    return data_sammon\n",
    "\n",
    "encode_methods = {\n",
    "    \"pca\": reduce_by_pca,\n",
    "    \"kpca\":reduce_by_kpca,\n",
    "    \"svd\": reduce_by_svd,\n",
    "    'mds':reduce_by_mds,\n",
    "    \"isomap\": reduce_by_isomap,\n",
    "    \"tsne\": reduce_by_tsne,\n",
    "    #\"sammon\": reduce_by_sammon\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for word in unique_word_list:\n",
    "    embeddings.update({word:{\"SBERT\":embedder.encode(word)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_vectors = [entry['SBERT'] for entry in embeddings.values()]\n",
    "matrix_2d = np.array(sbert_vectors)\n",
    "\n",
    "for new_dimension in range(1,37):\n",
    "    for method_name, reduction_func in encode_methods.items():\n",
    "        reduced_matrix = reduction_func(matrix_2d, new_dimension)  # Use the desired new_dimension\n",
    "\n",
    "        for i, (word, methods) in enumerate(embeddings.items()):\n",
    "            if 'SBERT' in methods:\n",
    "                embeddings[word][f'SBERT_{method_name}_{new_dimension}'] = reduced_matrix[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from CustomClasses.Models import W2VModel\\n\\ndimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\\nwindow = 5\\n\\nfor vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\\n\\nW2V = W2VModel(768, 5)\\nprint(\"First 10 dims of second word in 768 dims: \")\\nW2V.getvector(word_list[1])[:10]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from CustomClasses.Models import W2VModel\n",
    "\n",
    "dimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\n",
    "window = 5\n",
    "\n",
    "for vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\n",
    "\n",
    "W2V = W2VModel(768, 5)\n",
    "print(\"First 10 dims of second word in 768 dims: \")\n",
    "W2V.getvector(word_list[1])[:10]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_encoding_function(type_of_encoding):\n",
    "    if type_of_encoding == \"pca\":\n",
    "        return reduce_by_pca\n",
    "    elif type_of_encoding == \"kpca\":\n",
    "        return reduce_by_kpca\n",
    "    elif type_of_encoding == \"svd\":\n",
    "        return reduce_by_svd\n",
    "    elif type_of_encoding == \"mds\":\n",
    "        return reduce_by_mds\n",
    "    elif type_of_encoding == \"isomap\":\n",
    "        return reduce_by_isomap\n",
    "    elif type_of_encoding == \"tsne\":\n",
    "        return reduce_by_tsne\n",
    "    elif type_of_encoding == \"sammon\":\n",
    "        return reduce_by_sammon\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding type: {type_of_encoding}\")\n",
    "\n",
    "def get_word_dims(symbols):\n",
    "    word_dimensions = {}  # Dictionary to store word dimensions\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        parts = symbol.name.split(\"_\")\n",
    "        word = parts[0]  # Get the word part of the symbol\n",
    "        max_integer = int(parts[-1]) + 1\n",
    "        \n",
    "        if word not in word_dimensions or max_integer > word_dimensions[word]:\n",
    "            word_dimensions[word] = max_integer\n",
    "    \n",
    "    return word_dimensions\n",
    "    \n",
    "def check_model_exists_and_load(model_folder_path):\n",
    "    if os.path.exists(model_folder_path):\n",
    "        # If the model folder already exists, load the information from the files and return\n",
    "        model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "        training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "        validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "        model_params = joblib.load(model_params_filepath)\n",
    "        training_losses = np.load(training_losses_filepath)\n",
    "        validation_losses = np.load(validation_losses_filepath)\n",
    "\n",
    "        return model_params, training_losses, validation_losses, True\n",
    "    return None, None, None, False\n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "class QuantumEncodedNumpyModel(NumpyModel):\n",
    "    def initialize_weights(self):\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialized. Instantiate through ''`from_diagrams()`.')\n",
    "        word_dims = get_word_dims(self.symbols)\n",
    "        weights_list = []\n",
    "        for i, symbol in enumerate(self.symbols):\n",
    "            parts = symbol.name.split(\"_\")\n",
    "            word = parts[0]  # Get the word part of the symbol\n",
    "            encoding_method = self.type_of_encoding\n",
    "            if 'random' in encoding_method:\n",
    "                weight = random.uniform(-1, 1) * np.pi\n",
    "            elif \"uniform\" not in encoding_method and \"normal\" not in encoding_method:\n",
    "                weight = embeddings[word].get(encoding_method+\"_\"+str(word_dims[word]))[int(parts[-1])]\n",
    "            elif \"uniform\" in encoding_method:\n",
    "                #print(encoding_method.split(\"-\"))\n",
    "                method, value = encoding_method.split(\"-\")\n",
    "                weight = float(value)\n",
    "            elif \"normal\" in encoding_method:\n",
    "                if \"SBERT\" in encoding_method:\n",
    "                    bert_embbed = embeddings[word]['SBERT']\n",
    "                    mu = bert_embbed.mean()\n",
    "                    sig = bert_embbed.std()\n",
    "                    weight = np.random.normal(mu, sig, 1)[0]\n",
    "                if \"zero\" in encoding_method:\n",
    "                    #Normal with mu=0, sigma=1\n",
    "                    weight = np.random.normal(0, 1, 1)[0]\n",
    "            weights_list.append(weight/(np.pi))\n",
    "        self.weights = np.array(weights_list)\n",
    "\n",
    "\n",
    "def run_quantum_trainer(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text='text'):\n",
    "    noun_count = ansatz.ob_map[Ty('n')]\n",
    "    sentence_count = ansatz.ob_map[Ty('s')]\n",
    "    ansatz_hyperparams = {'n': noun_count, 's': sentence_count, 'layers': ansatz.n_layers}\n",
    "\n",
    "    ansatz_name = ansatz.__class__.__name__.lower()\n",
    "    loss_name = loss_function.__class__.__name__.lower()\n",
    "    optimizer_name = optimizer.__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in optim_hyperparams.items()])\n",
    "    ansatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in ansatz_hyperparams.items()])\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{ansatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"red_dim_models\", model_folder)\n",
    "\n",
    "    model_params, training_losses, validation_losses, dont_proceed_if_model_exists = check_model_exists_and_load(model_folder_path)\n",
    "    if dont_proceed_if_model_exists is True:\n",
    "        return model_params, training_losses, validation_losses\n",
    "    print(model_type, ansatz_name, ansatz_hyperparams_str, ansatz.n_layers, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    diagrams, train_dataset, val_dataset = get_datasets(ansatz, seed, batch_size)\n",
    "\n",
    "    model = QuantumEncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    model.type_of_encoding = model_type\n",
    "    model.initialize_weights()\n",
    "\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=loss_function,\n",
    "        epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        optim_hyperparams=optim_hyperparams,\n",
    "        evaluate_on_train=True,\n",
    "        verbose=text,\n",
    "        seed=seed\n",
    "    )\n",
    "    trainer.fit(train_dataset, val_dataset, logging_step=1000)\n",
    "\n",
    "    save_model_training(model_folder_path, model.weights, trainer.train_epoch_costs, trainer.val_costs)\n",
    "\n",
    "    return model_params, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_mds sim15ansatz n2_s2_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\discopy\\messages.py:95: UserWarning: Since discopy v0.4.3 the behaviour of permutation has changed. Pass inverse=False to get the default behaviour.\n",
      "  warnings.warn(message)\n",
      "Epoch 1:     train/loss: 0.2121   valid/loss: 0.1443\n",
      "Epoch 1000:  train/loss: 0.1943   valid/loss: 0.1435\n",
      "Epoch 2000:  train/loss: 0.1704   valid/loss: 0.1411\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_svd sim15ansatz n2_s2_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(model_type_config\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m     49\u001b[0m     ansatz_instance \u001b[39m=\u001b[39m ansatz_class(ansatz_param, n_layers\u001b[39m=\u001b[39mlayer)\n\u001b[1;32m---> 50\u001b[0m     run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m1.0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.01\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m0.01\u001b[39;49m\u001b[39m*\u001b[39;49mEPOCHS}, EPOCHS, BATCH_SIZE, SEED, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 127\u001b[0m, in \u001b[0;36mrun_quantum_trainer\u001b[1;34m(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text)\u001b[0m\n\u001b[0;32m    115\u001b[0m model\u001b[39m.\u001b[39minitialize_weights()\n\u001b[0;32m    117\u001b[0m trainer \u001b[39m=\u001b[39m QuantumTrainer(\n\u001b[0;32m    118\u001b[0m     model,\n\u001b[0;32m    119\u001b[0m     loss_function\u001b[39m=\u001b[39mloss_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     seed\u001b[39m=\u001b[39mseed\n\u001b[0;32m    126\u001b[0m )\n\u001b[1;32m--> 127\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_dataset, val_dataset, logging_step\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m    129\u001b[0m save_model_training(model_folder_path, model\u001b[39m.\u001b[39mweights, trainer\u001b[39m.\u001b[39mtrain_epoch_costs, trainer\u001b[39m.\u001b[39mval_costs)\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m model_params, training_losses, validation_losses\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_trainer.py:199\u001b[0m, in \u001b[0;36mQuantumTrainer.fit\u001b[1;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    192\u001b[0m         train_dataset: Dataset,\n\u001b[0;32m    193\u001b[0m         val_dataset: Dataset \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m         evaluation_step: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    195\u001b[0m         logging_step: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39m_training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(train_dataset, val_dataset, evaluation_step, logging_step)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39m_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\trainer.py:429\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39mfor\u001b[39;00m v_batch \u001b[39min\u001b[39;00m tqdm(val_dataset,\n\u001b[0;32m    423\u001b[0m                     desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation batch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    424\u001b[0m                     total\u001b[39m=\u001b[39mbatches_per_validation,\n\u001b[0;32m    425\u001b[0m                     disable\u001b[39m=\u001b[39mdisable_tqdm,\n\u001b[0;32m    426\u001b[0m                     leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    427\u001b[0m                     position\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m    428\u001b[0m     x_val, y_label_val \u001b[39m=\u001b[39m v_batch\n\u001b[1;32m--> 429\u001b[0m     y_hat_val, cur_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidation_step(v_batch)\n\u001b[0;32m    430\u001b[0m     val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_loss \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(x_val)\n\u001b[0;32m    431\u001b[0m     seen_so_far \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x_val)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_trainer.py:187\u001b[0m, in \u001b[0;36mQuantumTrainer.validation_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform a validation step.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \n\u001b[0;32m    175\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m--> 187\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m    188\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(y_hat, y)\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m y_hat, loss\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_model.py:146\u001b[0m, in \u001b[0;36mQuantumModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training:\n\u001b[0;32m    148\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_prediction(out)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:192\u001b[0m, in \u001b[0;36mNumpyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: \u001b[39mlist\u001b[39m[Diagram]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    175\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform default forward pass of a lambeq model.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[39m    In case of a different datapoint (e.g. list of tuple) or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_diagram_output(x)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:158\u001b[0m, in \u001b[0;36mNumpyModel.get_diagram_output\u001b[1;34m(self, diagrams)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m numpy \u001b[39mas\u001b[39;00m jnp\n\u001b[0;32m    157\u001b[0m     lambdified_diagrams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lambda(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m diagrams]\n\u001b[1;32m--> 158\u001b[0m     res: jnp\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([diag_f(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[0;32m    159\u001b[0m                                   \u001b[39mfor\u001b[39;00m diag_f \u001b[39min\u001b[39;00m lambdified_diagrams])\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m    163\u001b[0m diagrams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_subs(diagrams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:158\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m numpy \u001b[39mas\u001b[39;00m jnp\n\u001b[0;32m    157\u001b[0m     lambdified_diagrams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lambda(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m diagrams]\n\u001b[1;32m--> 158\u001b[0m     res: jnp\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([diag_f(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\n\u001b[0;32m    159\u001b[0m                                   \u001b[39mfor\u001b[39;00m diag_f \u001b[39min\u001b[39;00m lambdified_diagrams])\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m    163\u001b[0m diagrams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_subs(diagrams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\pjit.py:250\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39m@api_boundary\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcache_miss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 250\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[39m=\u001b[39m _python_pjit_helper(\n\u001b[0;32m    251\u001b[0m       fun, infer_params_fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    252\u001b[0m   executable \u001b[39m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[0;32m    253\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\pjit.py:163\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[1;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   dispatch\u001b[39m.\u001b[39mcheck_arg(arg)\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m   out_flat \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs_flat, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[0;32m   2674\u001b[0m                 default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m   2675\u001b[0m top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[0;32m   2676\u001b[0m              \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n\u001b[1;32m-> 2677\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[0;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m--> 815\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39mimpl(\u001b[39m*\u001b[39mtracers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\pjit.py:1203\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[1;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[0;32m   1200\u001b[0m donated_argnums \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(donated_invars) \u001b[39mif\u001b[39;00m d]\n\u001b[0;32m   1201\u001b[0m has_explicit_sharding \u001b[39m=\u001b[39m _pjit_explicit_sharding(\n\u001b[0;32m   1202\u001b[0m     in_shardings, out_shardings, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1203\u001b[0m \u001b[39mreturn\u001b[39;00m xc\u001b[39m.\u001b[39;49m_xla\u001b[39m.\u001b[39;49mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[0;32m   1204\u001b[0m                     _get_cpp_global_cache(has_explicit_sharding))(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\pjit.py:1187\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[1;34m(*args_, **kwargs_)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_impl_cache_miss\u001b[39m(\u001b[39m*\u001b[39margs_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_):\n\u001b[1;32m-> 1187\u001b[0m   out_flat, compiled \u001b[39m=\u001b[39m _pjit_call_impl_python(\n\u001b[0;32m   1188\u001b[0m       \u001b[39m*\u001b[39;49margs, jaxpr\u001b[39m=\u001b[39;49mjaxpr, in_shardings\u001b[39m=\u001b[39;49min_shardings,\n\u001b[0;32m   1189\u001b[0m       out_shardings\u001b[39m=\u001b[39;49mout_shardings, resource_env\u001b[39m=\u001b[39;49mresource_env,\n\u001b[0;32m   1190\u001b[0m       donated_invars\u001b[39m=\u001b[39;49mdonated_invars, name\u001b[39m=\u001b[39;49mname, keep_unused\u001b[39m=\u001b[39;49mkeep_unused,\n\u001b[0;32m   1191\u001b[0m       inline\u001b[39m=\u001b[39;49minline)\n\u001b[0;32m   1192\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(\n\u001b[0;32m   1193\u001b[0m       compiled, tree_structure(out_flat), args, out_flat)\n\u001b[0;32m   1194\u001b[0m   \u001b[39mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\pjit.py:1123\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[1;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[0;32m   1116\u001b[0m in_shardings \u001b[39m=\u001b[39m _resolve_in_shardings(\n\u001b[0;32m   1117\u001b[0m     args, in_shardings, out_shardings,\n\u001b[0;32m   1118\u001b[0m     resource_env\u001b[39m.\u001b[39mphysical_mesh \u001b[39mif\u001b[39;00m resource_env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m compiled \u001b[39m=\u001b[39m _pjit_lower(\n\u001b[0;32m   1121\u001b[0m     jaxpr, in_shardings, out_shardings, resource_env,\n\u001b[0;32m   1122\u001b[0m     donated_invars, name, keep_unused, inline,\n\u001b[1;32m-> 1123\u001b[0m     always_lower\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, lowering_platform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39;49mcompile()\n\u001b[0;32m   1124\u001b[0m _most_recent_pjit_call_executable\u001b[39m.\u001b[39mweak_key_dict[jaxpr] \u001b[39m=\u001b[39m compiled\n\u001b[0;32m   1125\u001b[0m \u001b[39m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2323\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[1;34m(self, compiler_options)\u001b[0m\n\u001b[0;32m   2320\u001b[0m   executable \u001b[39m=\u001b[39m MeshExecutable\u001b[39m.\u001b[39mfrom_trivial_jaxpr(\n\u001b[0;32m   2321\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile_args)\n\u001b[0;32m   2322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2323\u001b[0m   executable \u001b[39m=\u001b[39m UnloadedMeshExecutable\u001b[39m.\u001b[39mfrom_hlo(\n\u001b[0;32m   2324\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name,\n\u001b[0;32m   2325\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hlo,\n\u001b[0;32m   2326\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile_args,\n\u001b[0;32m   2327\u001b[0m       compiler_options\u001b[39m=\u001b[39mcompiler_options)\n\u001b[0;32m   2328\u001b[0m \u001b[39mif\u001b[39;00m compiler_options \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2329\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executable \u001b[39m=\u001b[39m executable\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2645\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   2642\u001b[0m       mesh \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mmesh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   2643\u001b[0m       \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 2645\u001b[0m xla_executable, compile_options \u001b[39m=\u001b[39m _cached_compilation(\n\u001b[0;32m   2646\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[0;32m   2647\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_outputs,\n\u001b[0;32m   2648\u001b[0m     \u001b[39mtuple\u001b[39;49m(host_callbacks), backend, da, pmap_nreps,\n\u001b[0;32m   2649\u001b[0m     compiler_options_keys, compiler_options_values)\n\u001b[0;32m   2651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(backend, \u001b[39m\"\u001b[39m\u001b[39mcompile_replicated\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   2652\u001b[0m   semantics_in_shardings \u001b[39m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2555\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[1;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[0;32m   2550\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, compile_options\n\u001b[0;32m   2552\u001b[0m \u001b[39mwith\u001b[39;00m dispatch\u001b[39m.\u001b[39mlog_elapsed_time(\n\u001b[0;32m   2553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFinished XLA compilation of \u001b[39m\u001b[39m{fun_name}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{elapsed_time}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2554\u001b[0m     fun_name\u001b[39m=\u001b[39mname, event\u001b[39m=\u001b[39mdispatch\u001b[39m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[1;32m-> 2555\u001b[0m   xla_executable \u001b[39m=\u001b[39m dispatch\u001b[39m.\u001b[39;49mcompile_or_get_cached(\n\u001b[0;32m   2556\u001b[0m       backend, computation, dev, compile_options, host_callbacks)\n\u001b[0;32m   2557\u001b[0m \u001b[39mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\dispatch.py:497\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[1;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[0;32m    493\u001b[0m use_compilation_cache \u001b[39m=\u001b[39m (compilation_cache\u001b[39m.\u001b[39mis_initialized() \u001b[39mand\u001b[39;00m\n\u001b[0;32m    494\u001b[0m                          backend\u001b[39m.\u001b[39mplatform \u001b[39min\u001b[39;00m supported_platforms)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_compilation_cache:\n\u001b[1;32m--> 497\u001b[0m   \u001b[39mreturn\u001b[39;00m backend_compile(backend, computation, compile_options,\n\u001b[0;32m    498\u001b[0m                          host_callbacks)\n\u001b[0;32m    500\u001b[0m cache_key \u001b[39m=\u001b[39m compilation_cache\u001b[39m.\u001b[39mget_cache_key(\n\u001b[0;32m    501\u001b[0m     computation, devices, compile_options, backend)\n\u001b[0;32m    503\u001b[0m cached_executable \u001b[39m=\u001b[39m _cache_read(module_name, cache_key, compile_options,\n\u001b[0;32m    504\u001b[0m                                 backend)\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[1;32mc:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\jax\\_src\\dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[1;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[0;32m    460\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m    461\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[0;32m    462\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ansatz_param_config = {\n",
    "    '1_1':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "    '2_1':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 1},\n",
    "    '1_2':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 2},\n",
    "    '2_2':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 2},\n",
    "    }\n",
    "\n",
    "layer_config = {\n",
    "    '1':1,\n",
    "    '2':2,\n",
    "    '3':3\n",
    "}\n",
    "\n",
    "ansatz_config = {\n",
    "        'IQP': IQPAnsatz,\n",
    "        'Sim14': Sim14Ansatz,\n",
    "        'Sim15': Sim15Ansatz,\n",
    "    }\n",
    "\n",
    "model_type_config = {\n",
    "    \"random\":\"random\",\n",
    "    \"SBERT_pca\": \"SBERT_pca\",\n",
    "    \"SBERT_kpca\": \"SBERT_kpca\",\n",
    "    \"SBERT_svd\":\"SBERT_svd\",\n",
    "    \"SBERT_mds\":\"SBERT_mds\",\n",
    "    \"SBERT_isomap\":\"SBERT_isomap\",\n",
    "    #\"SBERT_sammon\":\"SBERT_sammon\",\n",
    "    \"SBERT_normal\":\"SBERT_normal\",\n",
    "    #\"W2V_pca\": \"W2V_pca\",\n",
    "    #\"W2V_kpca\": \"W2V_kpca\",\n",
    "    #\"W2V_svd\":\"W2V_svd\",\n",
    "    #\"W2V_mds\":\"W2V_mds\",\n",
    "    #\"W2V_isomap\":\"W2V_isomap\",\n",
    "    #\"W2V_sammon\":\"W2V_sammon\",\n",
    "    #\"W2V_normal\":\"W2V_normal\",\n",
    "    \"uniform-0\":\"uniform-0\",\n",
    "    \"uniform-0.5\":\"uniform-0.5\",\n",
    "    \"uniform-1\":\"uniform-1\",\n",
    "\t\"zero_normal\":\"normal_zero\",\n",
    "}\n",
    "\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = len(pd.read_csv(\"Data/TrainingData.txt\"))\n",
    "SEED = 42\n",
    "for ansatz_name, ansatz_class in reversed(ansatz_config.items()):\n",
    "    for ansatz_param_name, ansatz_param in reversed(ansatz_param_config.items()):\n",
    "        for layer_name, layer in reversed(layer_config.items()):\n",
    "            for model_type in reversed(model_type_config.keys()):\n",
    "                ansatz_instance = ansatz_class(ansatz_param, n_layers=layer)\n",
    "                run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {'a': 1.0, 'c': 0.01, 'A':0.01*EPOCHS}, EPOCHS, BATCH_SIZE, SEED, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
