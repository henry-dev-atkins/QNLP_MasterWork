{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    return grouped_data, unique_word_list\n",
    "\n",
    "dataset, unique_word_list = read_and_preprocess_data()\n",
    "\n",
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def get_word_dims_from_ansatz(ANSATZ):\n",
    "    noun = ANSATZ.ob_map[Ty('n')]\n",
    "    sent = ANSATZ.ob_map[Ty('s')]\n",
    "    if isinstance(ANSATZ, IQPAnsatz):\n",
    "        noun_parameters = 3 if noun == 1 else (noun-1)\n",
    "        subject_parameters = noun + noun + sent - 1\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim14Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*4\n",
    "        subject_parameters = 4*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim15Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*2\n",
    "        subject_parameters = 2*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, StronglyEntanglingAnsatz):\n",
    "        print(\"ERROR NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "def get_datasets(ansatz, seed, batch_size):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=batch_size)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=batch_size)\n",
    "    return diagrams, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Reduced Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_pca(input_array, new_dims):\n",
    "    pca = PCA(n_components=new_dims+1)\n",
    "    pca.fit(input_array)\n",
    "    data_pca = pca.transform(input_array)\n",
    "    return data_pca\n",
    "\n",
    "def reduce_by_kpca(input_array, new_dims, kernel='rbf'):\n",
    "    kpca = KernelPCA(n_components=new_dims, kernel=kernel)\n",
    "    data_kpca = kpca.fit_transform(input_array)\n",
    "    return data_kpca\n",
    "\n",
    "def reduce_by_svd(input_array, new_dims):\n",
    "    U, D, Vt = np.linalg.svd(input_array)\n",
    "    U_reduced = U[:, :new_dims]\n",
    "    A_reduced = np.dot(U_reduced, np.diag(D[:new_dims]))\n",
    "    return A_reduced\n",
    "\n",
    "def reduce_by_mds(input_array, new_dims):\n",
    "    mds = MDS(n_components=new_dims, normalized_stress='auto')\n",
    "    data_mds = mds.fit_transform(input_array)\n",
    "    return data_mds\n",
    "\n",
    "def reduce_by_isomap(input_array, new_dims, n_neighbors=5):\n",
    "    pairwise_distances = squareform(pdist(input_array))\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=new_dims)\n",
    "    data_isomap = isomap.fit_transform(pairwise_distances)\n",
    "    return data_isomap\n",
    "\n",
    "def reduce_by_tsne(input_array, new_dims, perplexity=30, learning_rate=200):\n",
    "    tsne = TSNE(n_components=new_dims, perplexity=perplexity, learning_rate=learning_rate, method='exact')\n",
    "    data_tsne = tsne.fit_transform(input_array)\n",
    "    return data_tsne\n",
    "\n",
    "def sammon_mapping_loss(Y, X, delta):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    sum_delta = np.sum(delta)\n",
    "    d_ij = pdist(Y)\n",
    "    d_ij[d_ij == 0] = 1e-10  # Avoid division by zero\n",
    "    loss = np.sum((d_ij - delta) ** 2 / (d_ij * delta)) / (2 * sum_delta)\n",
    "    return loss\n",
    "\n",
    "def reduce_by_sammon(input_array, new_dims):\n",
    "    D = pdist(input_array)  # Calculate distance matrix using the original high-dimensional data\n",
    "    first_guess = np.random.rand(input_array.shape[0], new_dims)\n",
    "    print(first_guess.shape)\n",
    "    result = minimize(\n",
    "        lambda Y: sammon_mapping_loss(Y, input_array, D),\n",
    "        first_guess,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    data_sammon = result.x\n",
    "    return data_sammon\n",
    "\n",
    "encode_methods = {\n",
    "    \"pca\": reduce_by_pca,\n",
    "    \"kpca\":reduce_by_kpca,\n",
    "    \"svd\": reduce_by_svd,\n",
    "    'mds':reduce_by_mds,\n",
    "    \"isomap\": reduce_by_isomap,\n",
    "    \"tsne\": reduce_by_tsne,\n",
    "    #\"sammon\": reduce_by_sammon\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for word in unique_word_list:\n",
    "    embeddings.update({word:{\"SBERT\":embedder.encode(word)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_vectors = [entry['SBERT'] for entry in embeddings.values()]\n",
    "matrix_2d = np.array(sbert_vectors)\n",
    "\n",
    "#\n",
    "for new_dimension in range(2,37):\n",
    "    for method_name, reduction_func in encode_methods.items():\n",
    "        reduced_matrix = reduction_func(matrix_2d, new_dimension)  # Use the desired new_dimension\n",
    "\n",
    "        for i, (word, methods) in enumerate(embeddings.items()):\n",
    "            if 'SBERT' in methods:\n",
    "                embeddings[word][f'SBERT_{method_name}_{new_dimension}'] = reduced_matrix[i]\n",
    "# embeddings['level']['SBERT_isomap_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from CustomClasses.Models import W2VModel\\n\\ndimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\\nwindow = 5\\n\\nfor vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\\n\\nW2V = W2VModel(768, 5)\\nprint(\"First 10 dims of second word in 768 dims: \")\\nW2V.getvector(word_list[1])[:10]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from CustomClasses.Models import W2VModel\n",
    "\n",
    "dimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\n",
    "window = 5\n",
    "\n",
    "for vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\n",
    "\n",
    "W2V = W2VModel(768, 5)\n",
    "print(\"First 10 dims of second word in 768 dims: \")\n",
    "W2V.getvector(word_list[1])[:10]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_encoding_function(type_of_encoding):\n",
    "    if type_of_encoding == \"pca\":\n",
    "        return reduce_by_pca\n",
    "    elif type_of_encoding == \"kpca\":\n",
    "        return reduce_by_kpca\n",
    "    elif type_of_encoding == \"svd\":\n",
    "        return reduce_by_svd\n",
    "    elif type_of_encoding == \"mds\":\n",
    "        return reduce_by_mds\n",
    "    elif type_of_encoding == \"isomap\":\n",
    "        return reduce_by_isomap\n",
    "    elif type_of_encoding == \"tsne\":\n",
    "        return reduce_by_tsne\n",
    "    elif type_of_encoding == \"sammon\":\n",
    "        return reduce_by_sammon\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding type: {type_of_encoding}\")\n",
    "\n",
    "def get_word_dims(symbols):\n",
    "    word_dimensions = {}  # Dictionary to store word dimensions\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        parts = symbol.name.split(\"_\")\n",
    "        word = parts[0]  # Get the word part of the symbol\n",
    "        max_integer = int(parts[-1]) + 1\n",
    "        \n",
    "        if word not in word_dimensions or max_integer > word_dimensions[word]:\n",
    "            word_dimensions[word] = max_integer\n",
    "    \n",
    "    return word_dimensions\n",
    "    \n",
    "def check_model_exists_and_load(model_folder_path):\n",
    "    if os.path.exists(model_folder_path):\n",
    "        # If the model folder already exists, load the information from the files and return\n",
    "        model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "        training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "        validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "        model_params = joblib.load(model_params_filepath)\n",
    "        training_losses = np.load(training_losses_filepath)\n",
    "        validation_losses = np.load(validation_losses_filepath)\n",
    "\n",
    "        return model_params, training_losses, validation_losses, True\n",
    "    return None, None, None, False\n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "class QuantumEncodedNumpyModel(NumpyModel):\n",
    "    def initialize_weights(self):\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialized. Instantiate through ''`from_diagrams()`.')\n",
    "        word_dims = get_word_dims(self.symbols)\n",
    "        weights_list = []\n",
    "        for i, symbol in enumerate(self.symbols):\n",
    "            parts = symbol.name.split(\"_\")\n",
    "            word = parts[0]  # Get the word part of the symbol\n",
    "            encoding_method = self.type_of_encoding\n",
    "            if 'random' in encoding_method:\n",
    "                weight = random.uniform(-1, 1) * np.pi\n",
    "            elif \"uniform\" not in encoding_method and \"normal\" not in encoding_method:\n",
    "                weight = embeddings[word].get(encoding_method+\"_\"+str(word_dims[word]))[int(parts[-1])]\n",
    "            elif \"uniform\" in encoding_method:\n",
    "                #print(encoding_method.split(\"-\"))\n",
    "                method, value = encoding_method.split(\"-\")\n",
    "                weight = float(value)\n",
    "            elif \"normal\" in encoding_method:\n",
    "                if \"SBERT\" in encoding_method:\n",
    "                    bert_embbed = embeddings[word]['SBERT']\n",
    "                    mu = bert_embbed.mean()\n",
    "                    sig = bert_embbed.std()\n",
    "                    weight = np.random.normal(mu, sig, 1)[0]\n",
    "            weights_list.append(weight/(np.pi))\n",
    "        self.weights = np.array(weights_list)\n",
    "\n",
    "\n",
    "def run_quantum_trainer(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text='text'):\n",
    "    noun_count = ansatz.ob_map[Ty('n')]\n",
    "    sentence_count = ansatz.ob_map[Ty('s')]\n",
    "    ansatz_hyperparams = {'n': noun_count, 's': sentence_count, 'layers': ansatz.n_layers}\n",
    "\n",
    "    ansatz_name = ansatz.__class__.__name__.lower()\n",
    "    loss_name = loss_function.__class__.__name__.lower()\n",
    "    optimizer_name = optimizer.__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in optim_hyperparams.items()])\n",
    "    ansatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in ansatz_hyperparams.items()])\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{ansatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"red_dim_models\", model_folder)\n",
    "\n",
    "    model_params, training_losses, validation_losses, dont_proceed_if_model_exists = check_model_exists_and_load(model_folder_path)\n",
    "    if dont_proceed_if_model_exists is True:\n",
    "        return model_params, training_losses, validation_losses\n",
    "    print(model_type, ansatz_name, ansatz_hyperparams_str, ansatz.n_layers, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    diagrams, train_dataset, val_dataset = get_datasets(ansatz, seed, batch_size)\n",
    "\n",
    "    model = QuantumEncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    model.type_of_encoding = model_type\n",
    "    model.initialize_weights()\n",
    "\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=loss_function,\n",
    "        epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        optim_hyperparams=optim_hyperparams,\n",
    "        evaluate_on_train=True,\n",
    "        verbose=text,\n",
    "        seed=seed\n",
    "    )\n",
    "    trainer.fit(train_dataset, val_dataset, logging_step=1000)\n",
    "\n",
    "    save_model_training(model_folder_path, model.weights, trainer.train_epoch_costs, trainer.val_costs)\n",
    "\n",
    "    return model_params, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_svd iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1758   valid/loss: 0.1130\n",
      "Epoch 1000:  train/loss: 0.0353   valid/loss: 0.0965\n",
      "Epoch 2000:  train/loss: 0.0229   valid/loss: 0.0991\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_mds iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1931   valid/loss: 0.1291\n",
      "Epoch 1000:  train/loss: 0.0652   valid/loss: 0.1115\n",
      "Epoch 2000:  train/loss: 0.0444   valid/loss: 0.1097\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_isomap iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1968   valid/loss: 0.1280\n",
      "Epoch 1000:  train/loss: 0.0644   valid/loss: 0.0992\n",
      "Epoch 2000:  train/loss: 0.0326   valid/loss: 0.0927\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_normal iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1236   valid/loss: 0.0867\n",
      "Epoch 1000:  train/loss: 0.0134   valid/loss: 0.1010\n",
      "Epoch 2000:  train/loss: 0.0108   valid/loss: 0.1049\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform-0 iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.4135   valid/loss: 0.4905\n",
      "Epoch 1000:  train/loss: 0.4232   valid/loss: 0.4905\n",
      "Epoch 2000:  train/loss: 0.4233   valid/loss: 0.4905\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform-0.5 iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1940   valid/loss: 0.1284\n",
      "Epoch 1000:  train/loss: 0.0203   valid/loss: 0.1149\n",
      "Epoch 2000:  train/loss: 0.0163   valid/loss: 0.1138\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform-1 iqpansatz n1_s1_layers2 2 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(609,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.0697   valid/loss: 0.0441\n",
      "Epoch 1000:  train/loss: 0.0158   valid/loss: 0.0928\n",
      "Epoch 2000:  train/loss: 0.0123   valid/loss: 0.0988\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random iqpansatz n1_s1_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(735,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1892   valid/loss: 0.1241\n",
      "Epoch 1000:  train/loss: 0.0602   valid/loss: 0.1015\n",
      "Epoch 2000:  train/loss: 0.0334   valid/loss: 0.0932\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_pca iqpansatz n1_s1_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n",
      "(735,)\n"
     ]
    }
   ],
   "source": [
    "ansatz_param_config = {\n",
    "    '1_1':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "    '2_1':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 1},\n",
    "    '1_2':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 2},\n",
    "    '2_2':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 2},\n",
    "    }\n",
    "\n",
    "layer_config = {\n",
    "    '1':1,\n",
    "    '2':2,\n",
    "    '3':3\n",
    "}\n",
    "\n",
    "ansatz_config = {\n",
    "        'IQP': IQPAnsatz,\n",
    "        'Sim14': Sim14Ansatz,\n",
    "        'Sim15': Sim15Ansatz,\n",
    "    }\n",
    "\n",
    "model_type_config = {\n",
    "    \"random\":\"random\",\n",
    "    \"SBERT_pca\": \"SBERT_pca\",\n",
    "    \"SBERT_kpca\": \"SBERT_kpca\",\n",
    "    \"SBERT_svd\":\"SBERT_svd\",\n",
    "    \"SBERT_mds\":\"SBERT_mds\",\n",
    "    \"SBERT_isomap\":\"SBERT_isomap\",\n",
    "    #\"SBERT_sammon\":\"SBERT_sammon\",\n",
    "    \"SBERT_normal\":\"SBERT_normal\",\n",
    "    #\"W2V_pca\": \"W2V_pca\",\n",
    "    #\"W2V_kpca\": \"W2V_kpca\",\n",
    "    #\"W2V_svd\":\"W2V_svd\",\n",
    "    #\"W2V_mds\":\"W2V_mds\",\n",
    "    #\"W2V_isomap\":\"W2V_isomap\",\n",
    "    #\"W2V_sammon\":\"W2V_sammon\",\n",
    "    #\"W2V_normal\":\"W2V_normal\",\n",
    "    \"uniform-0\":\"uniform-0\",\n",
    "    \"uniform-0.5\":\"uniform-0.5\",\n",
    "    \"uniform-1\":\"uniform-1\",\n",
    "\t#\"normal_zero\":\"normal_zero\",\n",
    "}\n",
    "\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = len(pd.read_csv(\"Data/TrainingData.txt\"))\n",
    "SEED = 42\n",
    "for ansatz_name, ansatz_class in reversed(ansatz_config.items()):\n",
    "    for ansatz_param_name, ansatz_param in reversed(ansatz_param_config.items()):\n",
    "        for layer_name, layer in reversed(layer_config.items()):\n",
    "            for model_type in reversed(model_type_config.keys()):\n",
    "                ansatz_instance = ansatz_class(ansatz_param, n_layers=layer)\n",
    "                run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {'a': 1.0, 'c': 0.01, 'A':0.01*EPOCHS}, EPOCHS, BATCH_SIZE, SEED, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Random vs Uniform vs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
