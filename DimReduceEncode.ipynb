{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    return grouped_data, unique_word_list\n",
    "\n",
    "dataset, unique_word_list = read_and_preprocess_data()\n",
    "\n",
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def get_word_dims_from_ansatz(ANSATZ):\n",
    "    noun = ANSATZ.ob_map[Ty('n')]\n",
    "    sent = ANSATZ.ob_map[Ty('s')]\n",
    "    if isinstance(ANSATZ, IQPAnsatz):\n",
    "        noun_parameters = 3 if noun == 1 else (noun-1)\n",
    "        subject_parameters = noun + noun + sent - 1\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim14Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*4\n",
    "        subject_parameters = 4*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim15Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*2\n",
    "        subject_parameters = 2*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, StronglyEntanglingAnsatz):\n",
    "        print(\"ERROR NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "def get_datasets(ansatz, seed, batch_size):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=batch_size)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=batch_size)\n",
    "    return diagrams, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Reduced Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_pca(input_array, new_dims):\n",
    "    pca = PCA(n_components=new_dims+1)\n",
    "    pca.fit(input_array)\n",
    "    data_pca = pca.transform(input_array)\n",
    "    return data_pca\n",
    "\n",
    "def reduce_by_kpca(input_array, new_dims, kernel='rbf'):\n",
    "    kpca = KernelPCA(n_components=new_dims, kernel=kernel)\n",
    "    data_kpca = kpca.fit_transform(input_array)\n",
    "    return data_kpca\n",
    "\n",
    "def reduce_by_svd(input_array, new_dims):\n",
    "    U, D, Vt = np.linalg.svd(input_array)\n",
    "    U_reduced = U[:, :new_dims]\n",
    "    A_reduced = np.dot(U_reduced, np.diag(D[:new_dims]))\n",
    "    return A_reduced\n",
    "\n",
    "def reduce_by_mds(input_array, new_dims):\n",
    "    mds = MDS(n_components=new_dims, normalized_stress='auto')\n",
    "    data_mds = mds.fit_transform(input_array)\n",
    "    return data_mds\n",
    "\n",
    "def reduce_by_isomap(input_array, new_dims, n_neighbors=5):\n",
    "    pairwise_distances = squareform(pdist(input_array))\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=new_dims)\n",
    "    data_isomap = isomap.fit_transform(pairwise_distances)\n",
    "    return data_isomap\n",
    "\n",
    "def reduce_by_tsne(input_array, new_dims, perplexity=30, learning_rate=200):\n",
    "    tsne = TSNE(n_components=new_dims, perplexity=perplexity, learning_rate=learning_rate, method='exact')\n",
    "    data_tsne = tsne.fit_transform(input_array)\n",
    "    return data_tsne\n",
    "\n",
    "def sammon_mapping_loss(Y, X, delta):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    sum_delta = np.sum(delta)\n",
    "    d_ij = pdist(Y)\n",
    "    d_ij[d_ij == 0] = 1e-10  # Avoid division by zero\n",
    "    loss = np.sum((d_ij - delta) ** 2 / (d_ij * delta)) / (2 * sum_delta)\n",
    "    return loss\n",
    "\n",
    "def reduce_by_sammon(input_array, new_dims):\n",
    "    D = pdist(input_array)  # Calculate distance matrix using the original high-dimensional data\n",
    "    first_guess = np.random.rand(input_array.shape[0], new_dims)\n",
    "    print(first_guess.shape)\n",
    "    result = minimize(\n",
    "        lambda Y: sammon_mapping_loss(Y, input_array, D),\n",
    "        first_guess,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    data_sammon = result.x\n",
    "    return data_sammon\n",
    "\n",
    "encode_methods = {\n",
    "    \"pca\": reduce_by_pca,\n",
    "    \"kpca\":reduce_by_kpca,\n",
    "    \"svd\": reduce_by_svd,\n",
    "    'mds':reduce_by_mds,\n",
    "    \"isomap\": reduce_by_isomap,\n",
    "    \"tsne\": reduce_by_tsne,\n",
    "    #\"sammon\": reduce_by_sammon\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for word in unique_word_list:\n",
    "    embeddings.update({word:{\"SBERT\":embedder.encode(word)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m new_dimension \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m method_name, reduction_func \u001b[39min\u001b[39;00m encode_methods\u001b[39m.\u001b[39mitems():\n\u001b[1;32m----> 6\u001b[0m         reduced_matrix \u001b[39m=\u001b[39m reduction_func(matrix_2d, new_dimension)  \u001b[39m# Use the desired new_dimension\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m i, (word, methods) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(embeddings\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m      9\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSBERT\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m methods:\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mreduce_by_tsne\u001b[1;34m(input_array, new_dims, perplexity, learning_rate)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_by_tsne\u001b[39m(input_array, new_dims, perplexity\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m):\n\u001b[0;32m     30\u001b[0m     tsne \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39mnew_dims, perplexity\u001b[39m=\u001b[39mperplexity, learning_rate\u001b[39m=\u001b[39mlearning_rate, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexact\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     data_tsne \u001b[39m=\u001b[39m tsne\u001b[39m.\u001b[39;49mfit_transform(input_array)\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m data_tsne\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1119\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1119\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m   1120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m embedding\n\u001b[0;32m   1121\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1012\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[39m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[39m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m degrees_of_freedom \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tsne(\n\u001b[0;32m   1013\u001b[0m     P,\n\u001b[0;32m   1014\u001b[0m     degrees_of_freedom,\n\u001b[0;32m   1015\u001b[0m     n_samples,\n\u001b[0;32m   1016\u001b[0m     X_embedded\u001b[39m=\u001b[39;49mX_embedded,\n\u001b[0;32m   1017\u001b[0m     neighbors\u001b[39m=\u001b[39;49mneighbors_nn,\n\u001b[0;32m   1018\u001b[0m     skip_num_points\u001b[39m=\u001b[39;49mskip_num_points,\n\u001b[0;32m   1019\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1080\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m   1078\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[0;32m   1079\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mn_iter_without_progress\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_without_progress\n\u001b[1;32m-> 1080\u001b[0m     params, kl_divergence, it \u001b[39m=\u001b[39m _gradient_descent(obj_func, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopt_args)\n\u001b[0;32m   1082\u001b[0m \u001b[39m# Save the final number of iterations\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m it\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:399\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39m# only compute the error when needed\u001b[39;00m\n\u001b[0;32m    397\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompute_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m check_convergence \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m n_iter \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 399\u001b[0m error, grad \u001b[39m=\u001b[39m objective(p, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    401\u001b[0m inc \u001b[39m=\u001b[39m update \u001b[39m*\u001b[39m grad \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    402\u001b[0m dec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minvert(inc)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:196\u001b[0m, in \u001b[0;36m_kl_divergence\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, skip_num_points, compute_error)\u001b[0m\n\u001b[0;32m    194\u001b[0m PQd \u001b[39m=\u001b[39m squareform((P \u001b[39m-\u001b[39m Q) \u001b[39m*\u001b[39m dist)\n\u001b[0;32m    195\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(skip_num_points, n_samples):\n\u001b[1;32m--> 196\u001b[0m     grad[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(np\u001b[39m.\u001b[39;49mravel(PQd[i], order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m\"\u001b[39;49m), X_embedded[i] \u001b[39m-\u001b[39;49m X_embedded)\n\u001b[0;32m    197\u001b[0m grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39mravel()\n\u001b[0;32m    198\u001b[0m c \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m (degrees_of_freedom \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m degrees_of_freedom\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sbert_vectors = [entry['SBERT'] for entry in embeddings.values()]\n",
    "matrix_2d = np.array(sbert_vectors)\n",
    "\n",
    "for new_dimension in range(2,10):\n",
    "    for method_name, reduction_func in encode_methods.items():\n",
    "        reduced_matrix = reduction_func(matrix_2d, new_dimension)  # Use the desired new_dimension\n",
    "\n",
    "        for i, (word, methods) in enumerate(embeddings.items()):\n",
    "            if 'SBERT' in methods:\n",
    "                embeddings[word][f'SBERT_{method_name}_{new_dimension}'] = reduced_matrix[i]\n",
    "# embeddings['level']['SBERT_isomap_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from CustomClasses.Models import W2VModel\\n\\ndimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\\nwindow = 5\\n\\nfor vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\\n\\nW2V = W2VModel(768, 5)\\nprint(\"First 10 dims of second word in 768 dims: \")\\nW2V.getvector(word_list[1])[:10]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from CustomClasses.Models import W2VModel\n",
    "\n",
    "dimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\n",
    "window = 5\n",
    "\n",
    "for vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\n",
    "\n",
    "W2V = W2VModel(768, 5)\n",
    "print(\"First 10 dims of second word in 768 dims: \")\n",
    "W2V.getvector(word_list[1])[:10]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_encoding_function(type_of_encoding):\n",
    "    if type_of_encoding == \"pca\":\n",
    "        return reduce_by_pca\n",
    "    if type_of_encoding == \"kpca\":\n",
    "        return reduce_by_kpca\n",
    "    elif type_of_encoding == \"svd\":\n",
    "        return reduce_by_svd\n",
    "    elif type_of_encoding == \"mds\":\n",
    "        return reduce_by_mds\n",
    "    elif type_of_encoding == \"isomap\":\n",
    "        return reduce_by_isomap\n",
    "    elif type_of_encoding == \"tsne\":\n",
    "        return reduce_by_tsne\n",
    "    elif type_of_encoding == \"sammon\":\n",
    "        return reduce_by_sammon\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding type: {type_of_encoding}\")\n",
    "\n",
    "def get_word_dims(symbols):\n",
    "    word_dimensions = {}  # Dictionary to store word dimensions\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        parts = symbol.name.split(\"_\")\n",
    "        word = parts[0]  # Get the word part of the symbol\n",
    "        max_integer = int(parts[-1]) + 1\n",
    "        \n",
    "        if word not in word_dimensions or max_integer > word_dimensions[word]:\n",
    "            word_dimensions[word] = max_integer\n",
    "    \n",
    "    return word_dimensions\n",
    "    \n",
    "def check_model_exists_and_load(model_folder_path):\n",
    "    if os.path.exists(model_folder_path):\n",
    "        # If the model folder already exists, load the information from the files and return\n",
    "        model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "        training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "        validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "        model_params = joblib.load(model_params_filepath)\n",
    "        training_losses = np.load(training_losses_filepath)\n",
    "        validation_losses = np.load(validation_losses_filepath)\n",
    "\n",
    "        return model_params, training_losses, validation_losses, True\n",
    "    return None, None, None, False\n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "class QuantumEncodedNumpyModel(NumpyModel):\n",
    "    def initialize_weights(self):\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialized. Instantiate through ''`from_diagrams()`.')\n",
    "        word_dims = get_word_dims(self.symbols)\n",
    "        weights_list = []\n",
    "        for i, symbol in enumerate(self.symbols):\n",
    "            parts = symbol.name.split(\"_\")\n",
    "            word = parts[0]  # Get the word part of the symbol\n",
    "            encoding_method = self.type_of_encoding\n",
    "            weight = embeddings[word].get(encoding_method+\"_\"+str(word_dims[word]))[int(parts[-1])]\n",
    "            weights_list.append(weight/(np.pi))\n",
    "        self.weights = np.array(weights_list)\n",
    "\n",
    "\n",
    "def run_quantum_trainer(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text='text'):\n",
    "    noun_count = ansatz.ob_map[Ty('n')]\n",
    "    sentence_count = ansatz.ob_map[Ty('s')]\n",
    "    ansatz_hyperparams = {'n': noun_count, 's': sentence_count, 'layers': ansatz.n_layers}\n",
    "\n",
    "    ansatz_name = ansatz.__class__.__name__.lower()\n",
    "    loss_name = loss_function.__class__.__name__.lower()\n",
    "    optimizer_name = optimizer.__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in optim_hyperparams.items()])\n",
    "    ansatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in ansatz_hyperparams.items()])\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{ansatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"red_dim_models\", model_folder)\n",
    "\n",
    "    model_params, training_losses, validation_losses, dont_proceed_if_model_exists = check_model_exists_and_load(model_folder_path)\n",
    "    if dont_proceed_if_model_exists is True:\n",
    "        return model_params, training_losses, validation_losses\n",
    "    print(model_type, ansatz_name, ansatz_hyperparams_str, ansatz.n_layers, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    diagrams, train_dataset, val_dataset = get_datasets(ansatz, seed, batch_size)\n",
    "\n",
    "    ### Model Assignment according to function inputs\n",
    "    if model_type == \"random\":\n",
    "        model = NumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    else:\n",
    "        model = QuantumEncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        model.type_of_encoding = model_type\n",
    "        model.initialize_weights()\n",
    "\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=loss_function,\n",
    "        epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        optim_hyperparams=optim_hyperparams,\n",
    "        evaluate_on_train=True,\n",
    "        verbose=text,\n",
    "        seed=seed\n",
    "    )\n",
    "    trainer.fit(train_dataset, val_dataset, logging_step=500)\n",
    "\n",
    "    save_model_training(model_folder_path, model.weights, trainer.train_epoch_costs, trainer.val_costs)\n",
    "\n",
    "    return model_params, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random iqpansatz n1_s1_layers1 1 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    }
   ],
   "source": [
    "ansatz_param_config = {\n",
    "    '1_1':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "    '2_1':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 1},\n",
    "    '1_2':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 2},\n",
    "    '2_2':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 2},\n",
    "    }\n",
    "\n",
    "layer_config = {\n",
    "    '1':1,\n",
    "    '2':2,\n",
    "    '3':3\n",
    "}\n",
    "\n",
    "ansatz_config = {\n",
    "        'IQP': IQPAnsatz,\n",
    "        'Sim14': Sim14Ansatz,\n",
    "        'Sim15': Sim15Ansatz,\n",
    "    }\n",
    "\n",
    "model_type_config = {\n",
    "    \"random\":\"random\",\n",
    "    \"SBERT_pca\": \"SBERT_pca\",\n",
    "    \"SBERT_kpca\": \"SBERT_kpca\",\n",
    "    \"SBERT_svd\":\"SBERT_svd\",\n",
    "    \"SBERT_mds\":\"SBERT_mds\",\n",
    "    \"SBERT_isomap\":\"SBERT_isomap\",\n",
    "    #\"SBERT_sammon\":\"SBERT_sammon\",\n",
    "    #\"SBERT_normal\":\"SBERT_normal\",\n",
    "    #\"W2V_pca\": \"W2V_pca\",\n",
    "    #\"W2V_kpca\": \"W2V_kpca\",\n",
    "    #\"W2V_svd\":\"W2V_svd\",\n",
    "    #\"W2V_mds\":\"W2V_mds\",\n",
    "    #\"W2V_isomap\":\"W2V_isomap\",\n",
    "    #\"W2V_sammon\":\"W2V_sammon\",\n",
    "    #\"W2V_normal\":\"W2V_normal\",\n",
    "    #\"uniformZero\":\"uniformZero\",\n",
    "    #\"uniformHalf\":\"uniformHalf\",\n",
    "    #\"uniformOne\":\"uniformOne\",\n",
    "}\n",
    "\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = len(pd.read_csv(\"Data/TrainingData.txt\"))\n",
    "SEED = 42\n",
    "for ansatz_name, ansatz_class in ansatz_config.items():  \n",
    "    for ansatz_param_name, ansatz_param in ansatz_param_config.items():\n",
    "        for layer_name, layer in layer_config.items():\n",
    "            for model_type in model_type_config.keys():\n",
    "                ansatz_instance = ansatz_class(ansatz_param, n_layers=layer)  \n",
    "                run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {'a': 1.0, 'c': 0.01, 'A':0.01*EPOCHS}, EPOCHS, BATCH_SIZE, SEED, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
