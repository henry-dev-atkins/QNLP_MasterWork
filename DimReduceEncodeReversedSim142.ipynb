{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine, pdist, squareform\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    return grouped_data, unique_word_list\n",
    "\n",
    "dataset, unique_word_list = read_and_preprocess_data()\n",
    "\n",
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def get_word_dims_from_ansatz(ANSATZ):\n",
    "    noun = ANSATZ.ob_map[Ty('n')]\n",
    "    sent = ANSATZ.ob_map[Ty('s')]\n",
    "    if isinstance(ANSATZ, IQPAnsatz):\n",
    "        noun_parameters = 3 if noun == 1 else (noun-1)\n",
    "        subject_parameters = noun + noun + sent - 1\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim14Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*4\n",
    "        subject_parameters = 4*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim15Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*2\n",
    "        subject_parameters = 2*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, StronglyEntanglingAnsatz):\n",
    "        print(\"ERROR NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "def get_datasets(ansatz, seed, batch_size):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=batch_size)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=batch_size)\n",
    "    return diagrams, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Reduced Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_pca(input_array, new_dims):\n",
    "    pca = PCA(n_components=new_dims)\n",
    "    pca.fit(input_array)\n",
    "    data_pca = pca.transform(input_array)\n",
    "    return data_pca\n",
    "\n",
    "def reduce_by_kpca(input_array, new_dims, kernel='rbf'):\n",
    "    kpca = KernelPCA(n_components=new_dims, kernel=kernel)\n",
    "    data_kpca = kpca.fit_transform(input_array)\n",
    "    return data_kpca\n",
    "\n",
    "def reduce_by_svd(input_array, new_dims):\n",
    "    U, D, Vt = np.linalg.svd(input_array)\n",
    "    U_reduced = U[:, :new_dims]\n",
    "    A_reduced = np.dot(U_reduced, np.diag(D[:new_dims]))\n",
    "    return A_reduced\n",
    "\n",
    "def reduce_by_mds(input_array, new_dims):\n",
    "    mds = MDS(n_components=new_dims, normalized_stress='auto')\n",
    "    data_mds = mds.fit_transform(input_array)\n",
    "    return data_mds\n",
    "\n",
    "def reduce_by_isomap(input_array, new_dims, n_neighbors=5):\n",
    "    pairwise_distances = squareform(pdist(input_array))\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=new_dims)\n",
    "    data_isomap = isomap.fit_transform(pairwise_distances)\n",
    "    return data_isomap\n",
    "\n",
    "def reduce_by_tsne(input_array, new_dims, perplexity=30, learning_rate=200):\n",
    "    tsne = TSNE(n_components=new_dims, perplexity=perplexity, learning_rate=learning_rate, method='exact')\n",
    "    data_tsne = tsne.fit_transform(input_array)\n",
    "    return data_tsne\n",
    "\n",
    "def sammon_mapping_loss(Y, X, delta):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    sum_delta = np.sum(delta)\n",
    "    d_ij = pdist(Y)\n",
    "    d_ij[d_ij == 0] = 1e-10  # Avoid division by zero\n",
    "    loss = np.sum((d_ij - delta) ** 2 / (d_ij * delta)) / (2 * sum_delta)\n",
    "    return loss\n",
    "\n",
    "def reduce_by_sammon(input_array, new_dims):\n",
    "    D = pdist(input_array)  # Calculate distance matrix using the original high-dimensional data\n",
    "    first_guess = np.random.rand(input_array.shape[0], new_dims)\n",
    "    print(first_guess.shape)\n",
    "    result = minimize(\n",
    "        lambda Y: sammon_mapping_loss(Y, input_array, D),\n",
    "        first_guess,\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    data_sammon = result.x\n",
    "    return data_sammon\n",
    "\n",
    "encode_methods = {\n",
    "    \"pca\": reduce_by_pca,\n",
    "    \"kpca\":reduce_by_kpca,\n",
    "    \"svd\": reduce_by_svd,\n",
    "    'mds':reduce_by_mds,\n",
    "    \"isomap\": reduce_by_isomap,\n",
    "    \"tsne\": reduce_by_tsne,\n",
    "    #\"sammon\": reduce_by_sammon\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for word in unique_word_list:\n",
    "    embeddings.update({word:{\"SBERT\":embedder.encode(word)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_vectors = [entry['SBERT'] for entry in embeddings.values()]\n",
    "matrix_2d = np.array(sbert_vectors)\n",
    "\n",
    "for new_dimension in range(1,37):\n",
    "    for method_name, reduction_func in encode_methods.items():\n",
    "        reduced_matrix = reduction_func(matrix_2d, new_dimension)  # Use the desired new_dimension\n",
    "\n",
    "        for i, (word, methods) in enumerate(embeddings.items()):\n",
    "            if 'SBERT' in methods:\n",
    "                embeddings[word][f'SBERT_{method_name}_{new_dimension}'] = reduced_matrix[i]\n",
    "\n",
    "#embeddings['level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from CustomClasses.Models import W2VModel\\n\\ndimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\\nwindow = 5\\n\\nfor vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\\n\\nW2V = W2VModel(768, 5)\\nprint(\"First 10 dims of second word in 768 dims: \")\\nW2V.getvector(word_list[1])[:10]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from CustomClasses.Models import W2VModel\n",
    "\n",
    "dimensions = [10,20,30,40,50,60,70,80,90] + list(np.arange(100,800,100))\n",
    "window = 5\n",
    "\n",
    "for vector_dims in dimensions: W2VModel(vector_dims, window, printing=True)\n",
    "\n",
    "W2V = W2VModel(768, 5)\n",
    "print(\"First 10 dims of second word in 768 dims: \")\n",
    "W2V.getvector(word_list[1])[:10]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_encoding_function(type_of_encoding):\n",
    "    if type_of_encoding == \"pca\":\n",
    "        return reduce_by_pca\n",
    "    elif type_of_encoding == \"kpca\":\n",
    "        return reduce_by_kpca\n",
    "    elif type_of_encoding == \"svd\":\n",
    "        return reduce_by_svd\n",
    "    elif type_of_encoding == \"mds\":\n",
    "        return reduce_by_mds\n",
    "    elif type_of_encoding == \"isomap\":\n",
    "        return reduce_by_isomap\n",
    "    elif type_of_encoding == \"tsne\":\n",
    "        return reduce_by_tsne\n",
    "    elif type_of_encoding == \"sammon\":\n",
    "        return reduce_by_sammon\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding type: {type_of_encoding}\")\n",
    "\n",
    "def get_word_dims(symbols):\n",
    "    word_dimensions = {}  # Dictionary to store word dimensions\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        parts = symbol.name.split(\"_\")\n",
    "        word = parts[0]  # Get the word part of the symbol\n",
    "        max_integer = int(parts[-1]) + 1\n",
    "        \n",
    "        if word not in word_dimensions or max_integer > word_dimensions[word]:\n",
    "            word_dimensions[word] = max_integer\n",
    "    \n",
    "    return word_dimensions\n",
    "    \n",
    "def check_model_exists_and_load(model_folder_path):\n",
    "    if os.path.exists(model_folder_path):\n",
    "        # If the model folder already exists, load the information from the files and return\n",
    "        model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "        training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "        validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "        model_params = joblib.load(model_params_filepath)\n",
    "        training_losses = np.load(training_losses_filepath)\n",
    "        validation_losses = np.load(validation_losses_filepath)\n",
    "\n",
    "        return model_params, training_losses, validation_losses, True\n",
    "    return None, None, None, False\n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "class QuantumEncodedNumpyModel(NumpyModel):\n",
    "    def initialize_weights(self):\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialized. Instantiate through ''`from_diagrams()`.')\n",
    "        word_dims = get_word_dims(self.symbols)\n",
    "        weights_list = []\n",
    "        for i, symbol in enumerate(self.symbols):\n",
    "            parts = symbol.name.split(\"_\")\n",
    "            word = parts[0]  # Get the word part of the symbol\n",
    "            encoding_method = self.type_of_encoding\n",
    "            if 'random' in encoding_method:\n",
    "                weight = random.uniform(-1, 1) * np.pi\n",
    "            elif \"uniform\" not in encoding_method and \"normal\" not in encoding_method:\n",
    "                weight = embeddings[word].get(encoding_method+\"_\"+str(word_dims[word]))[int(parts[-1])]\n",
    "            elif \"uniform\" in encoding_method:\n",
    "                #print(encoding_method.split(\"-\"))\n",
    "                method, value = encoding_method.split(\"-\")\n",
    "                weight = float(value)\n",
    "            elif \"normal\" in encoding_method:\n",
    "                if \"SBERT\" in encoding_method:\n",
    "                    bert_embbed = embeddings[word]['SBERT']\n",
    "                    mu = bert_embbed.mean()\n",
    "                    sig = bert_embbed.std()\n",
    "                    weight = np.random.normal(mu, sig, 1)[0]\n",
    "                if \"zero\" in encoding_method:\n",
    "                    #Normal with mu=0, sigma=1\n",
    "                    weight = np.random.normal(0, 1, 1)[0]\n",
    "            weights_list.append(weight/(np.pi))\n",
    "        self.weights = np.array(weights_list)\n",
    "\n",
    "\n",
    "def run_quantum_trainer(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text='text'):\n",
    "    noun_count = ansatz.ob_map[Ty('n')]\n",
    "    sentence_count = ansatz.ob_map[Ty('s')]\n",
    "    ansatz_hyperparams = {'n': noun_count, 's': sentence_count, 'layers': ansatz.n_layers}\n",
    "\n",
    "    ansatz_name = ansatz.__class__.__name__.lower()\n",
    "    loss_name = loss_function.__class__.__name__.lower()\n",
    "    optimizer_name = optimizer.__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in optim_hyperparams.items()])\n",
    "    ansatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in ansatz_hyperparams.items()])\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{ansatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"red_dim_models\", model_folder)\n",
    "\n",
    "    model_params, training_losses, validation_losses, dont_proceed_if_model_exists = check_model_exists_and_load(model_folder_path)\n",
    "    if dont_proceed_if_model_exists is True:\n",
    "        return model_params, training_losses, validation_losses\n",
    "    print(model_type, ansatz_name, ansatz_hyperparams_str, ansatz.n_layers, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    diagrams, train_dataset, val_dataset = get_datasets(ansatz, seed, batch_size)\n",
    "\n",
    "    model = QuantumEncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    model.type_of_encoding = model_type\n",
    "    model.initialize_weights()\n",
    "\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=loss_function,\n",
    "        epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        optim_hyperparams=optim_hyperparams,\n",
    "        evaluate_on_train=True,\n",
    "        verbose=text,\n",
    "        seed=seed\n",
    "    )\n",
    "    trainer.fit(train_dataset, val_dataset, logging_step=1000)\n",
    "\n",
    "    save_model_training(model_folder_path, model.weights, trainer.train_epoch_costs, trainer.val_costs)\n",
    "\n",
    "    return model_params, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_mds iqpansatz n1_s2_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.2023   valid/loss: 0.1344\n",
      "Epoch 1000:  train/loss: 0.1057   valid/loss: 0.1253\n",
      "Epoch 2000:  train/loss: 0.0772   valid/loss: 0.1197\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_isomap iqpansatz n1_s2_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.2002   valid/loss: 0.1360\n",
      "Epoch 1000:  train/loss: 0.1336   valid/loss: 0.1260\n",
      "Epoch 2000:  train/loss: 0.1058   valid/loss: 0.1191\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT_normal iqpansatz n1_s2_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1926   valid/loss: 0.1239\n",
      "Epoch 1000:  train/loss: 0.0537   valid/loss: 0.0739\n",
      "Epoch 2000:  train/loss: 0.0294   valid/loss: 0.0845\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform-0 iqpansatz n1_s2_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.1675   valid/loss: 0.1079\n",
      "Epoch 1000:  train/loss: 0.1677   valid/loss: 0.1079\n",
      "Epoch 2000:  train/loss: 0.1676   valid/loss: 0.1079\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform-0.5 iqpansatz n1_s2_layers3 3 mseloss spsaoptimizer a1.0_c0.01_A20.0 2000 74 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.0586   valid/loss: 0.0409\n",
      "Epoch 1000:  train/loss: 0.0169   valid/loss: 0.0837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m model_type_config\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     49\u001b[0m     ansatz_instance \u001b[39m=\u001b[39m ansatz_class(ansatz_param, n_layers\u001b[39m=\u001b[39mlayer)\n\u001b[1;32m---> 50\u001b[0m     run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m1.0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.01\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m0.01\u001b[39;49m\u001b[39m*\u001b[39;49mEPOCHS}, EPOCHS, BATCH_SIZE, SEED, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 127\u001b[0m, in \u001b[0;36mrun_quantum_trainer\u001b[1;34m(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text)\u001b[0m\n\u001b[0;32m    115\u001b[0m model\u001b[39m.\u001b[39minitialize_weights()\n\u001b[0;32m    117\u001b[0m trainer \u001b[39m=\u001b[39m QuantumTrainer(\n\u001b[0;32m    118\u001b[0m     model,\n\u001b[0;32m    119\u001b[0m     loss_function\u001b[39m=\u001b[39mloss_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     seed\u001b[39m=\u001b[39mseed\n\u001b[0;32m    126\u001b[0m )\n\u001b[1;32m--> 127\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_dataset, val_dataset, logging_step\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m    129\u001b[0m save_model_training(model_folder_path, model\u001b[39m.\u001b[39mweights, trainer\u001b[39m.\u001b[39mtrain_epoch_costs, trainer\u001b[39m.\u001b[39mval_costs)\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m model_params, training_losses, validation_losses\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_trainer.py:199\u001b[0m, in \u001b[0;36mQuantumTrainer.fit\u001b[1;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    192\u001b[0m         train_dataset: Dataset,\n\u001b[0;32m    193\u001b[0m         val_dataset: Dataset \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m         evaluation_step: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    195\u001b[0m         logging_step: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39m_training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(train_dataset, val_dataset, evaluation_step, logging_step)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39m_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\trainer.py:431\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39mfor\u001b[39;00m v_batch \u001b[39min\u001b[39;00m tqdm(val_dataset,\n\u001b[0;32m    425\u001b[0m                     desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation batch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    426\u001b[0m                     total\u001b[39m=\u001b[39mbatches_per_validation,\n\u001b[0;32m    427\u001b[0m                     disable\u001b[39m=\u001b[39mdisable_tqdm,\n\u001b[0;32m    428\u001b[0m                     leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    429\u001b[0m                     position\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m    430\u001b[0m     x_val, y_label_val \u001b[39m=\u001b[39m v_batch\n\u001b[1;32m--> 431\u001b[0m     y_hat_val, cur_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidation_step(v_batch)\n\u001b[0;32m    432\u001b[0m     val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_loss \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(x_val)\n\u001b[0;32m    433\u001b[0m     seen_so_far \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x_val)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_trainer.py:187\u001b[0m, in \u001b[0;36mQuantumTrainer.validation_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform a validation step.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \n\u001b[0;32m    175\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m--> 187\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m    188\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(y_hat, y)\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m y_hat, loss\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\quantum_model.py:146\u001b[0m, in \u001b[0;36mQuantumModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training:\n\u001b[0;32m    148\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_prediction(out)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:192\u001b[0m, in \u001b[0;36mNumpyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: \u001b[39mlist\u001b[39m[Diagram]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    175\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform default forward pass of a lambeq model.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[39m    In case of a different datapoint (e.g. list of tuple) or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_diagram_output(x)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:157\u001b[0m, in \u001b[0;36mNumpyModel.get_diagram_output\u001b[1;34m(self, diagrams)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_jit:\n\u001b[0;32m    155\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m numpy \u001b[39mas\u001b[39;00m jnp\n\u001b[1;32m--> 157\u001b[0m     lambdified_diagrams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lambda(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m diagrams]\n\u001b[0;32m    158\u001b[0m     res: jnp\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([diag_f(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[0;32m    159\u001b[0m                                   \u001b[39mfor\u001b[39;00m diag_f \u001b[39min\u001b[39;00m lambdified_diagrams])\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_jit:\n\u001b[0;32m    155\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m numpy \u001b[39mas\u001b[39;00m jnp\n\u001b[1;32m--> 157\u001b[0m     lambdified_diagrams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_lambda(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m diagrams]\n\u001b[0;32m    158\u001b[0m     res: jnp\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([diag_f(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[0;32m    159\u001b[0m                                   \u001b[39mfor\u001b[39;00m diag_f \u001b[39min\u001b[39;00m lambdified_diagrams])\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\lambeq\\training\\numpy_model.py:79\u001b[0m, in \u001b[0;36mNumpyModel._get_lambda\u001b[1;34m(self, diagram)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msymbols:\n\u001b[0;32m     77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mSymbols not initialised. Instantiate through \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     78\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m`NumpyModel.from_diagrams()`.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[39mif\u001b[39;00m diagram \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlambdas:\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambdas[diagram]\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiagram_output\u001b[39m(x: Iterable[ArrayLike]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\monoidal.py:486\u001b[0m, in \u001b[0;36mDiagram.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 486\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39m(\u001b[39mrepr\u001b[39;49m(\u001b[39mself\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\quantum\\circuit.py:177\u001b[0m, in \u001b[0;36mCircuit.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__repr__\u001b[39;49m()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mDiagram\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCircuit\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\monoidal.py:483\u001b[0m, in \u001b[0;36mDiagram.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboxes) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdom \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboxes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdom:\n\u001b[0;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboxes[\u001b[39m0\u001b[39m])  \u001b[39m# i.e. self is a generator.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDiagram(dom=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, cod=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, boxes=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, offsets=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    482\u001b[0m     \u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdom), \u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcod),\n\u001b[1;32m--> 483\u001b[0m     \u001b[39mrepr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboxes), \u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffsets))\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\quantum\\gates.py:331\u001b[0m, in \u001b[0;36mControlled.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39min\u001b[39;00m GATES:\n\u001b[0;32m    330\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mControlled(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrolled\u001b[39m}\u001b[39;00m\u001b[39m, distance=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistance\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\cat.py:598\u001b[0m, in \u001b[0;36mBox.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname) \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m[::-1]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dagger \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\discopy\\quantum\\gates.py:466\u001b[0m, in \u001b[0;36mParametrized.name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mname\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 466\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m(\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, format_number(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata))\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sympy\\core\\expr.py:394\u001b[0m, in \u001b[0;36mExpr.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[39mif\u001b[39;00m rounded\u001b[39m.\u001b[39mis_Float:\n\u001b[0;32m    393\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m(rounded, format_spec)\n\u001b[1;32m--> 394\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__format__\u001b[39;49m(format_spec)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sympy\\core\\_print_helpers.py:29\u001b[0m, in \u001b[0;36mPrintable.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprinting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstr\u001b[39;00m \u001b[39mimport\u001b[39;00m sstr\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m sstr(\u001b[39mself\u001b[39;49m, order\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sympy\\printing\\printer.py:372\u001b[0m, in \u001b[0;36m_PrintFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sympy\\printing\\str.py:997\u001b[0m, in \u001b[0;36msstr\u001b[1;34m(expr, **settings)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[39m@print_function\u001b[39m(StrPrinter)\n\u001b[0;32m    981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msstr\u001b[39m(expr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msettings):\n\u001b[0;32m    982\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the expression as a string.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[0;32m    984\u001b[0m \u001b[39m    For large expressions where speed is a concern, use the setting\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[39m    'Eq(a + b, 0)'\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 997\u001b[0m     p \u001b[39m=\u001b[39m StrPrinter(settings)\n\u001b[0;32m    998\u001b[0m     s \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mdoprint(expr)\n\u001b[0;32m   1000\u001b[0m     \u001b[39mreturn\u001b[39;00m s\n",
      "File \u001b[1;32mc:\\Users\\henry\\Desktop\\MastersProject\\quantum_env\\lib\\site-packages\\sympy\\printing\\printer.py:267\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m settings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_settings):\n\u001b[0;32m    268\u001b[0m         \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings:\n\u001b[0;32m    269\u001b[0m             \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_settings:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ansatz_param_config = {\n",
    "    '1_1':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "    '2_1':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 1},\n",
    "    '1_2':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 2},\n",
    "    '2_2':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 2},\n",
    "    }\n",
    "\n",
    "layer_config = {\n",
    "    #'1':1,\n",
    "    '2':2,\n",
    "    #'3':3\n",
    "}\n",
    "\n",
    "ansatz_config = {\n",
    "        #'IQP': IQPAnsatz,\n",
    "        'Sim14': Sim14Ansatz,\n",
    "        #'Sim15': Sim15Ansatz,\n",
    "    }\n",
    "\n",
    "model_type_config = {\n",
    "    \"random\":\"random\",\n",
    "    \"SBERT_pca\": \"SBERT_pca\",\n",
    "    \"SBERT_kpca\": \"SBERT_kpca\",\n",
    "    \"SBERT_svd\":\"SBERT_svd\",\n",
    "    \"SBERT_mds\":\"SBERT_mds\",\n",
    "    \"SBERT_isomap\":\"SBERT_isomap\",\n",
    "    #\"SBERT_sammon\":\"SBERT_sammon\",\n",
    "    \"SBERT_normal\":\"SBERT_normal\",\n",
    "    #\"W2V_pca\": \"W2V_pca\",\n",
    "    #\"W2V_kpca\": \"W2V_kpca\",\n",
    "    #\"W2V_svd\":\"W2V_svd\",\n",
    "    #\"W2V_mds\":\"W2V_mds\",\n",
    "    #\"W2V_isomap\":\"W2V_isomap\",\n",
    "    #\"W2V_sammon\":\"W2V_sammon\",\n",
    "    #\"W2V_normal\":\"W2V_normal\",\n",
    "    \"uniform-0\":\"uniform-0\",\n",
    "    \"uniform-0.5\":\"uniform-0.5\",\n",
    "    \"uniform-1\":\"uniform-1\",\n",
    "\t\"zero_normal\":\"normal_zero\",\n",
    "}\n",
    "\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = len(pd.read_csv(\"Data/TrainingData.txt\"))\n",
    "SEED = 42\n",
    "for ansatz_name, ansatz_class in ansatz_config.items():\n",
    "    for ansatz_param_name, ansatz_param in ansatz_param_config.items():\n",
    "        for layer_name, layer in layer_config.items():\n",
    "            for model_type in model_type_config.keys():\n",
    "                ansatz_instance = ansatz_class(ansatz_param, n_layers=layer)\n",
    "                run_quantum_trainer(model_type, ansatz_instance, MSELoss(), SPSAOptimizer, {'a': 1.0, 'c': 0.01, 'A':0.01*EPOCHS}, EPOCHS, BATCH_SIZE, SEED, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
