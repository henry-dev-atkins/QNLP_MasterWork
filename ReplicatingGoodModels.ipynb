{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "jax.devices()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "import torch\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "import jax as jnp\n",
    "from jax import numpy as jnp\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list))\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # Rescale the 'score' column\n",
    "    #grouped_data['score'] = scaler.fit_transform(grouped_data[['score']])\n",
    "    return grouped_data, unique_word_list\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit\n",
    "\n",
    "dataset, unique_word_list = read_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ansatz, BATCH_SIZE):\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return diagrams, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedNumpyModel(NumpyModel):\n",
    "     def initialise_weights(self) -> None:\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialised. Instantiate through '\n",
    "                            '`from_diagrams()`.')\n",
    "        self.weights = self.param_initialise_method(self.symbols)\n",
    "\n",
    "def model_with_parameters(parameters):\n",
    "    model_type = parameters['model_type']\n",
    "    noun_count = parameters['ansatz'].ob_map[Ty('n')]\n",
    "    ansatz = parameters['ansatz']\n",
    "    sentence_count = parameters['ansatz'].ob_map[Ty('s')]\n",
    "    asnatz_hyperparams = {'n':noun_count, 's':sentence_count, 'layers':parameters['ansatz'].n_layers}\n",
    "    ansatz_name = parameters['ansatz'].__class__.__name__.lower()\n",
    "    loss_name = parameters['loss'].__class__.__name__.lower()\n",
    "    optimizer_name = parameters['optimiser'].__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in parameters['optimiser_parameters'].items()])\n",
    "    asnatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in asnatz_hyperparams.items()])\n",
    "    num_epochs = parameters['epochs']\n",
    "    batch_size = parameters['batch']\n",
    "    seed = parameters['seed']\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{asnatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"models\", model_folder)\n",
    "    if os.path.exists(model_folder_path):\n",
    "        return 0\n",
    "    else:\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        diagrams, train_dataset, val_dataset = get_datasets(ansatz, batch_size)\n",
    "        model = make_and_train_model(diagrams, train_dataset, val_dataset, model_type, noun_count, ansatz, sentence_count, asnatz_hyperparams, ansatz_name, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "        \n",
    "        trainer = QuantumTrainer(\n",
    "            model,\n",
    "            loss_function=parameters['loss'],\n",
    "            epochs=num_epochs,\n",
    "            optimizer=parameters['optimiser'],\n",
    "            optim_hyperparams=parameters['optimiser_parameters'],\n",
    "            evaluate_on_train=True,\n",
    "            verbose=\"text\",\n",
    "            seed=seed\n",
    "        )\n",
    "        trainer.fit(train_dataset, val_dataset, logging_step=100)\n",
    "\n",
    "        save_model_training(model_folder_path, parameters, trainer.train_costs, trainer.val_costs)\n",
    "    return \n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "def make_and_train_model(diagrams, train_dataset, val_dataset, model_type, noun_count, ansatz, sentence_count, asnatz_hyperparams, ansatz_name, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed):\n",
    "    if model_type == 'random':\n",
    "        model = NumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid ansatz: {ansatz_name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:    train/loss: 0.1864   valid/loss: 0.1205\n",
      "Epoch 100:  train/loss: 0.0633   valid/loss: 0.0981\n",
      "Epoch 200:  train/loss: 0.0440   valid/loss: 0.1016\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "model_with_parameters({\n",
    "    \"model_type\":'random',\n",
    "    \"ansatz\":IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1}, n_layers=3),\n",
    "    \"loss\":MSELoss(),\n",
    "    \"optimiser\":SPSAOptimizer,\n",
    "    \"optimiser_parameters\":{'a': 0.75, 'c': 0.1, 'A':0.01*200},\n",
    "    \"seed\":42,\n",
    "    \"batch\":2,\n",
    "    \"epochs\":200\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  train/loss: 0.1882   valid/loss: 0.1259\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "model_with_parameters({\n",
    "    \"model_type\":'random',\n",
    "    \"ansatz\":IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1}, n_layers=3),\n",
    "    \"loss\":MSELoss(),\n",
    "    \"optimiser\":SPSAOptimizer,\n",
    "    \"optimiser_parameters\":{'a': 1.75, 'c': 0.1, 'A':0.01*2},\n",
    "    \"seed\":42,\n",
    "    \"batch\":2,\n",
    "    \"epochs\":2\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
