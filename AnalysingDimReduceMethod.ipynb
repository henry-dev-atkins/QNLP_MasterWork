{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([CpuDevice(id=0)],\n",
       " False,\n",
       " ['c:\\\\Users\\\\henry\\\\Desktop\\\\MastersProject\\\\quantum_env\\\\lib\\\\site-packages\\\\jaxlib'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jaxlib\n",
    "\n",
    "\n",
    "jax.devices(), torch.cuda.is_available(), jaxlib.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check GPU availability in PyTorch\n",
    "torch_available = torch.cuda.is_available()\n",
    "if torch_available:\n",
    "    print(\"PyTorch GPU Available\")\n",
    "\n",
    "# Check GPU availability in JAX\n",
    "jax_devices = jax.devices()\n",
    "if jax_devices:\n",
    "    print(\"JAX Devices:\", jax_devices)\n",
    "else:\n",
    "    print(\"No JAX Devices Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry\\Desktop\\A\\QNLP_MasterWork\\quantum_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)001fa/.gitattributes: 100%|██████████| 690/690 [00:00<?, ?B/s] \n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading (…)3bbb8001fa/README.md: 100%|██████████| 3.69k/3.69k [00:00<?, ?B/s]\n",
      "Downloading (…)bb8001fa/config.json: 100%|██████████| 629/629 [00:00<00:00, 630kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 122kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:30<00:00, 3.01MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 53.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)001fa/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.75MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 314/314 [00:00<?, ?B/s] \n",
      "Downloading (…)3bbb8001fa/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.20MB/s]\n",
      "Downloading (…)b8001fa/modules.json: 100%|██████████| 229/229 [00:00<?, ?B/s] \n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    return grouped_data, unique_word_list\n",
    "dataset, unique_word_list = read_and_preprocess_data()\n",
    "\n",
    "embeddings = {}\n",
    "for word in unique_word_list:\n",
    "    embeddings.update({word:{\"SBERT\":embedder.encode(word)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings and Reduced Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_by_pca(input_array, new_dims):\n",
    "    pca = PCA(n_components=new_dims)\n",
    "    pca.fit(input_array)\n",
    "    data_pca = pca.transform(input_array)\n",
    "    return data_pca\n",
    "\n",
    "def reduce_by_svd(input_array, new_dims):\n",
    "    U, D, Vt = np.linalg.svd(input_array)\n",
    "    U_reduced = U[:, :new_dims]\n",
    "    A_reduced = np.dot(U_reduced, np.diag(D[:new_dims]))\n",
    "    return A_reduced\n",
    "\n",
    "def reduce_by_lda(input_array, new_dims, labels):\n",
    "    lda = LDA(n_components=new_dims)\n",
    "    data_lda = lda.fit_transform(input_array, labels)\n",
    "    return data_lda\n",
    "\n",
    "def reduce_by_mds(input_array, new_dims):\n",
    "    pass\n",
    "\n",
    "def reduce_by_isomap(input_array, new_dims, n_neighbors=5):\n",
    "    pairwise_distances = squareform(pdist(input_array))\n",
    "    isomap = Isomap(n_neighbors=n_neighbors, n_components=new_dims)\n",
    "    data_isomap = isomap.fit_transform(pairwise_distances)\n",
    "    return data_isomap\n",
    "\n",
    "def reduce_by_tsne(input_array, new_dims, perplexity=30, learning_rate=200):\n",
    "    tsne = TSNE(n_components=new_dims, perplexity=perplexity, learning_rate=learning_rate)\n",
    "    data_tsne = tsne.fit_transform(input_array)\n",
    "    return data_tsne\n",
    "\n",
    "def sammon_mapping_loss(Y, X, delta):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    sum_delta = np.sum(delta)\n",
    "    d_ij = pdist(Y)\n",
    "    d_ij[d_ij == 0] = 1e-10  # Avoid division by zero\n",
    "    loss = np.sum((d_ij - delta) ** 2 / (d_ij * delta)) / (2 * sum_delta)\n",
    "    return loss\n",
    "\n",
    "def reduce_by_sammon(input_array, new_dims):\n",
    "    D = pdist(input_array)  # Calculate distance matrix using the original high-dimensional data\n",
    "    result = minimize(\n",
    "        lambda Y: sammon_mapping_loss(Y, input_array, D),\n",
    "        np.random.rand(input_array.shape[0], new_dims),\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "    data_sammon = result.x\n",
    "    return data_sammon\n",
    "\n",
    "input_vector_data = np.array([vector_dict['SBERT'] for word, vector_dict in embeddings.items()])\n",
    "sbert_vectors = [value['SBERT'].T for value in embeddings.values()]\n",
    "input_matrix_data = np.vstack(sbert_vectors) #For Sammon\n",
    "\n",
    "encode_methods = {\n",
    "    \"pca\": reduce_by_pca,\n",
    "    \"svd\": reduce_by_svd,\n",
    "    'mds':reduce_by_mds,\n",
    "    \"isomap\": reduce_by_isomap,\n",
    "    \"tsne\": reduce_by_tsne,\n",
    "    #\"sammon\": reduce_by_sammon\n",
    "}\n",
    "\n",
    "# Loop through the reduction methods and apply each one\n",
    "for method, reduce_func in encode_methods.items():\n",
    "    # Update the method name for consistency (e.g., \"pca\" becomes \"PCA\")\n",
    "    method_name = method.upper()\n",
    "    \n",
    "    # Specify dimensions based on the reduction method\n",
    "    if method == \"tsne\":\n",
    "        dims_list = [2, 3]  # t-SNE is typically used with 2 or 3 dimensions\n",
    "    else:\n",
    "        dims_list = range(1,20)  # For other methods, you can use the dimensions you want\n",
    "        \n",
    "    # Loop through the desired dimensions for reduction\n",
    "    for dim in dims_list:\n",
    "        # Construct the key for the embeddings dictionary\n",
    "        key = f\"{method}_{dim}\"\n",
    "        \n",
    "        # Apply the reduction method\n",
    "        reduced_data = reduce_func(input_vector_data, new_dims=dim)\n",
    "        \n",
    "        # Update the embeddings dictionary with the reduced data\n",
    "        for i, (word, vector_dict) in enumerate(embeddings.items()):\n",
    "            vector_dict[key] = reduced_data[i]\n",
    "embeddings['land'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEncodedNumpyModel(NumpyModel):#Rename to \"EncodedNumpyModel\"\n",
    "     def initialise_weights(self) -> None:\n",
    "        \"\"\"Initialise the weights of the model.\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `model.symbols` are not initialised.\n",
    "        \"\"\"\n",
    "        if self.type_of_encoding == \"pca\":\n",
    "            if not self.symbols:\n",
    "                raise ValueError('Symbols not initialised. Instantiate through '\n",
    "                                '`from_diagrams()`.')\n",
    "            self.weights = free_symbols_to_rotations(self.symbols)\n",
    "\n",
    "        if self.type_of_encoding == \"normal_distribution\":\n",
    "            if not self.symbols:\n",
    "                raise ValueError('Symbols not initialised. Instantiate through '\n",
    "                                '`from_diagrams()`.')\n",
    "            self.weights = normal_distribution_to_rotations(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_to_rotations(symbols):\n",
    "    noun_parameters, subject_parameters = get_word_dims_from_ansatz(ANSATZ)\n",
    "\n",
    "    weights = np.zeros(shape=(len(symbols)))\n",
    "    for i, word_symbol in enumerate(symbols):\n",
    "        word_string, word_dims, word_index = retrive_word_param_from_symbols(word_symbol, noun_parameters, subject_parameters)\n",
    "        weights[i] = word_vector_dict[word_string]['pca_'+str(word_dims)][word_index]/(2*np.pi)\n",
    "    return weights\n",
    "\n",
    "def normal_distribution_to_rotations(symbols):\n",
    "    noun_parameters, subject_parameters = get_word_dims_from_ansatz(ANSATZ)\n",
    "\n",
    "    weights = np.zeros(shape=(len(symbols)))\n",
    "    for i, word_symbol in enumerate(symbols):\n",
    "        word_string, word_dims, word_index = retrive_word_param_from_symbols(word_symbol, noun_parameters, subject_parameters)\n",
    "        \n",
    "        mean_of_word = word_vector_dict[word_string][384].mean()\n",
    "        std_of_word = word_vector_dict[word_string][384].std()\n",
    "        s = np.random.normal(mean_of_word, std_of_word, 1)\n",
    "\n",
    "        weights[i] = s\n",
    "    return weights\n",
    "\n",
    "def uniform_to_rotations(symbols):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_train_model(diagrams, train_dataset, val_dataset, model_type, noun_count, ansatz, sentence_count, asnatz_hyperparams, ansatz_name, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed):\n",
    "    if model_type == 'random':\n",
    "        model = NumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    elif model_type == 'pca':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        model.type_of_encoding = 'pca'\n",
    "        #model.param_initialise_method = pca_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'normal':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'uniform_zero':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'uniform_half':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'uniform_one':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'svd':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    elif model_type == 'normal_around_half':\n",
    "        model = EncodedNumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "        #model.param_initialise_method = normal_around_half_distribution_to_rotations\n",
    "        model.initialise_weights()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid ansatz: {ansatz_name}\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
