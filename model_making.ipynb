{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from discopy.grammar import Word\n",
    "from discopy.rigid import Cup, Id, Ty\n",
    "import torch\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from lambeq import LossFunction, PennyLaneModel, PytorchTrainer, QuantumTrainer, SPSAOptimizer, NumpyModel, MSELoss, Dataset, AtomicType, IQPAnsatz, Sim14Ansatz, Sim15Ansatz, StronglyEntanglingAnsatz, BobcatParser\n",
    "from lambeq.pregroups import remove_cups\n",
    "import jax as jnp\n",
    "\n",
    "\n",
    "from jax import numpy as jnp\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data():\n",
    "    df = pd.read_csv('Data/LargerSadrKartTransative.txt', sep=' ')\n",
    "    # assign column names to the dataframe\n",
    "    df.columns = ['annotator', 'subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2', 'score']\n",
    "    # group the data by the three sentence columns and calculate the mean and standard deviation of the score column\n",
    "    grouped_data = df.groupby(['subject1', 'verb1', 'object1', 'subject2', 'verb2', 'object2']).agg({'score': [np.mean, np.std]}).reset_index()\n",
    "    # flatten the multi-level column names of the grouped data\n",
    "    grouped_data.columns = [' '.join(col).strip() for col in grouped_data.columns.values]\n",
    "    # rename the mean and std columns to 'score' and 'range' respectively\n",
    "    grouped_data.rename(columns={'score mean': 'score', 'score std': 'range'}, inplace=True)\n",
    "    grouped_data['score'] = grouped_data['score']/grouped_data['score'].max()\n",
    "    unique_word_list = []\n",
    "    for ind, row in grouped_data.iterrows():\n",
    "        for i in [row['subject1'],row['verb1'],row['object1'], row['subject2'],row['verb2'],row['object2']]:\n",
    "            unique_word_list.append(i)\n",
    "    unique_word_list = list(set(unique_word_list)) #Makes word_list from word_list's unique elements\n",
    "    grouped_data.to_csv(\"Data/AveragedLargerSadrKartTransative.txt\")\n",
    "    # Create an instance of MinMaxScaler\n",
    "    #ERROR: SCALING STOPPED ALL CONVERGENCES!\n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # Rescale the 'score' column\n",
    "    #grouped_data['score'] = scaler.fit_transform(grouped_data[['score']])\n",
    "    return grouped_data, unique_word_list\n",
    "dataset, unique_word_list = read_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_nth_rows_sentences(data, row1, row2=None):\n",
    "    if not row2:\n",
    "        row2=row1\n",
    "    sentence1 = data['subject'+str(1)][row1] + \" \" + data['verb'+str(1)][row1]  + \" \" + data['object'+str(1)][row1] \n",
    "    sentence2 = data['subject'+str(2)][row2] + \" \" + data['verb'+str(2)][row2]  + \" \" + data['object'+str(2)][row2] \n",
    "    return sentence1, sentence2\n",
    "\n",
    "def make_sentence_a_state(sentence):\n",
    "    diagram = diagram_to_sentence(sentence.split(\" \"))\n",
    "    diagram = remove_cups(diagram)\n",
    "    return diagram\n",
    "\n",
    "def make_diagram_a_circuit(diagram, ansatz, dagger=False):\n",
    "    discopy_circuit = ansatz(diagram)\n",
    "    if dagger:\n",
    "        discopy_circuit = discopy_circuit.dagger()\n",
    "    return discopy_circuit\n",
    "\n",
    "def concat_circuits_into_inner_product(circuit1, circuit2):\n",
    "    concat_circuit = circuit1 >> circuit2\n",
    "    return concat_circuit\n",
    "\n",
    "def make_diagrams(data, sentence1, sentence2=None):\n",
    "    if type(sentence1) == int:\n",
    "        sentence1, sentence2 = retrive_nth_rows_sentences(data, sentence1, sentence2)\n",
    "    diagram1 = make_sentence_a_state(sentence1)\n",
    "    diagram2 = make_sentence_a_state(sentence2)\n",
    "    return diagram1, diagram2\n",
    "\n",
    "def diagram_to_sentence(word_list):\n",
    "    n, s = Ty('n'), Ty('s')\n",
    "    words = [\n",
    "        Word(word_list[0], n),\n",
    "        Word(word_list[1], n.r @ s @ n.l),\n",
    "        Word(word_list[2], n)\n",
    "    ]\n",
    "    cups = Cup(n, n.r) @ Id(s) @ Cup(n.l, n)\n",
    "    assert Id().tensor(*words) == words[0] @ words[1] @ words[2]\n",
    "    assert Ty().tensor(*[n.r, s, n.l]) == n.r @ s @ n.l\n",
    "    diagram = Id().tensor(*words) >> cups\n",
    "    return diagram\n",
    "\n",
    "def get_word_dims_from_ansatz(ANSATZ):\n",
    "    noun = ANSATZ.ob_map[Ty('n')]\n",
    "    sent = ANSATZ.ob_map[Ty('s')]\n",
    "    if isinstance(ANSATZ, IQPAnsatz):\n",
    "        noun_parameters = 3 if noun == 1 else (noun-1)\n",
    "        subject_parameters = noun + noun + sent - 1\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim14Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*4\n",
    "        subject_parameters = 4*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, Sim15Ansatz):\n",
    "        noun_parameters = 3 if noun == 1 else noun*2\n",
    "        subject_parameters = 2*(noun + noun + sent)\n",
    "        return noun_parameters, subject_parameters\n",
    "    if isinstance(ANSATZ, StronglyEntanglingAnsatz):\n",
    "        print(\"ERROR NOT IMPLEMENTED YET\")\n",
    "        pass\n",
    "\n",
    "def make_circuit_from_diagrams(diagram1, diagram2, ansatz, drawing=False):\n",
    "    discopy_circuit1 = make_diagram_a_circuit(diagram1, ansatz)\n",
    "    discopy_circuit2 = make_diagram_a_circuit(diagram2, ansatz, dagger=True)\n",
    "    discopy_circuit = concat_circuits_into_inner_product(discopy_circuit1, discopy_circuit2)\n",
    "\n",
    "    if drawing:\n",
    "        discopy_circuit1.draw(figsize=(5, 5))\n",
    "        discopy_circuit2.draw(figsize=(5, 5))\n",
    "        discopy_circuit.draw(figsize=(5, 10))   \n",
    "\n",
    "    pennylane_circuit = discopy_circuit.to_pennylane()\n",
    "    return pennylane_circuit, discopy_circuit\n",
    "\n",
    "def make_circuit_from_df_row(data, row_number, ansatz):\n",
    "    diagram1, diagram2 = make_diagrams(data, row_number)\n",
    "    qml_circuit, discopy_circuit = make_circuit_from_diagrams(diagram1, diagram2, ansatz, False)\n",
    "    return qml_circuit, discopy_circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(ansatz, seed, batch_size):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    labels = dataset['score']\n",
    "\n",
    "    training = pd.read_csv(\"Data/TrainingData.txt\")\n",
    "    test = pd.read_csv(\"Data/TestData.txt\")\n",
    "\n",
    "    train_data =  [make_circuit_from_df_row(training, i, ansatz)[1] for i in range(len(training))]\n",
    "    train_labels = labels[training['Unnamed: 0'].values]\n",
    "    val_data = [make_circuit_from_df_row(test, i, ansatz)[1] for i in range(len(test))] \n",
    "    val_labels = labels[test['Unnamed: 0'].values]\n",
    "\n",
    "    diagrams = train_data + val_data\n",
    "\n",
    "    train_dataset = Dataset(train_data,train_labels,batch_size=batch_size)\n",
    "    val_dataset = Dataset(val_data, val_labels, batch_size=batch_size)\n",
    "    return diagrams, train_dataset, val_dataset\n",
    "\n",
    "class EncodedNumpyModel(NumpyModel):\n",
    "     def initialise_weights(self) -> None:\n",
    "        if not self.symbols:\n",
    "            raise ValueError('Symbols not initialised. Instantiate through '\n",
    "                            '`from_diagrams()`.')\n",
    "        self.weights = self.param_initialise_method(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists_and_load(model_folder_path):\n",
    "    if os.path.exists(model_folder_path):\n",
    "        # If the model folder already exists, load the information from the files and return\n",
    "        model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "        training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "        validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "        model_params = joblib.load(model_params_filepath)\n",
    "        training_losses = np.load(training_losses_filepath)\n",
    "        validation_losses = np.load(validation_losses_filepath)\n",
    "\n",
    "        return model_params, training_losses, validation_losses, True\n",
    "    return None, None, None, False\n",
    "\n",
    "def save_model_training(model_folder_path, model_params, training_losses, validation_losses):\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "    model_params_filepath = os.path.join(model_folder_path, \"model_params.joblib\")\n",
    "    training_losses_filepath = os.path.join(model_folder_path, \"training_losses.npy\")\n",
    "    validation_losses_filepath = os.path.join(model_folder_path, \"validation_losses.npy\")\n",
    "\n",
    "    joblib.dump(model_params, model_params_filepath)\n",
    "    np.save(training_losses_filepath, training_losses)\n",
    "    np.save(validation_losses_filepath, validation_losses)\n",
    "    return\n",
    "\n",
    "def run_quantum_trainer(model_type, ansatz, loss_function, optimizer, optim_hyperparams, num_epochs, batch_size, seed, text='text'):\n",
    "    ### Preperation ###\n",
    "    noun_count = ansatz.ob_map[Ty('n')]\n",
    "    sentence_count = ansatz.ob_map[Ty('s')]\n",
    "    asnatz_hyperparams = {'n':noun_count, 's':sentence_count, 'layers':ansatz.n_layers}\n",
    "    \n",
    "    \n",
    "    ansatz_name = ansatz.__class__.__name__.lower()\n",
    "    loss_name = loss_function.__class__.__name__.lower()\n",
    "    optimizer_name = optimizer.__name__.lower()\n",
    "    optimizer_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in optim_hyperparams.items()])\n",
    "    asnatz_hyperparams_str = '_'.join([f\"{key}{val}\" for key, val in asnatz_hyperparams.items()])\n",
    "\n",
    "    model_folder = f\"{model_type}_{ansatz_name}_{asnatz_hyperparams_str}_{loss_name}_{optimizer_name}_{optimizer_hyperparams_str}_epochs{num_epochs}_batch{batch_size}_seed{seed}\"\n",
    "    model_folder_path = os.path.join(\"models\", model_folder)\n",
    "\n",
    "    model_params, training_losses, validation_losses, dont_proceed_if_model_exists = check_model_exists_and_load(model_folder_path)\n",
    "    if dont_proceed_if_model_exists is True:\n",
    "        return model_params, training_losses, validation_losses\n",
    "    print(ansatz_name, asnatz_hyperparams_str, ansatz.n_layers, loss_name, optimizer_name, optimizer_hyperparams_str, num_epochs, batch_size, seed)\n",
    "    \n",
    "    ### Data Produced\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    diagrams, train_dataset, val_dataset = get_datasets(ansatz, seed, batch_size)\n",
    "    \n",
    "    ### Model Assignment according to function inputs\n",
    "    if model_type == 'random':\n",
    "        model = NumpyModel.from_diagrams(diagrams, use_jit=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid ansatz: {ansatz_name}\")\n",
    "\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=loss_function,\n",
    "        epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        optim_hyperparams=optim_hyperparams,\n",
    "        evaluate_on_train=True,\n",
    "        verbose=text,\n",
    "        seed=seed\n",
    "    )\n",
    "    trainer.fit(train_dataset, val_dataset, logging_step=500)\n",
    "\n",
    "    save_model_training(model_folder_path, model.weights, trainer.train_epoch_costs, trainer.val_costs)\n",
    "\n",
    "    return model_params, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim15ansatz n2_s1_layers3 3 mseloss spsaoptimizer a0.01_c0.1_A5.0 500 2 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry\\Desktop\\A\\quantum_env\\lib\\site-packages\\discopy\\messages.py:95: UserWarning: Since discopy v0.4.3 the behaviour of permutation has changed. Pass inverse=False to get the default behaviour.\n",
      "  warnings.warn(message)\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Epoch 1:    train/loss: 0.2113   valid/loss: 0.1445\n",
      "Epoch 500:  train/loss: 0.2116   valid/loss: 0.1445\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim15ansatz n2_s1_layers3 3 mseloss spsaoptimizer a0.01_c0.01_A5.0 500 2 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:    train/loss: 0.2114   valid/loss: 0.1445\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'quantum_env (Python 3.10.7)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "ansatz_param_config = {\n",
    "    '1_1':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 1},\n",
    "    '2_1':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 1},\n",
    "    '1_2':{AtomicType.NOUN: 1, AtomicType.SENTENCE: 2},\n",
    "    '2_2':{AtomicType.NOUN: 2, AtomicType.SENTENCE: 2},\n",
    "    }\n",
    "\n",
    "optimizer_param_config = {\n",
    "    '0.01_0.01_'+str(0.01*EPOCHS): {'a': 0.01, 'c': 0.01, 'A':0.01*EPOCHS},\n",
    "    '0.01_0.1_'+str(0.01*EPOCHS): {'a': 0.01, 'c': 0.1, 'A':0.01*EPOCHS},\n",
    "    '0.01_1.0_'+str(0.01*EPOCHS): {'a': 0.01, 'c': 1.0, 'A':0.01*EPOCHS},\n",
    "    '0.1_0.01_'+str(0.01*EPOCHS): {'a': 0.1, 'c': 0.01, 'A':0.01*EPOCHS},\n",
    "    '0.1_0.1_'+str(0.01*EPOCHS): {'a': 0.1, 'c': 0.1, 'A':0.01*EPOCHS},\n",
    "    '0.1_1.0_'+str(0.01*EPOCHS): {'a': 0.1, 'c': 1.0, 'A':0.01*EPOCHS},\n",
    "    '1.0_0.01_'+str(0.01*EPOCHS): {'a': 1.0, 'c': 0.01, 'A':0.01*EPOCHS},\n",
    "    '1.0_0.1_'+str(0.01*EPOCHS): {'a': 1.0, 'c': 0.1, 'A':0.01*EPOCHS},\n",
    "    '1.0_1.0_'+str(0.01*EPOCHS): {'a': 1.0, 'c': 1.0, 'A':0.01*EPOCHS},\n",
    "    }\n",
    "\n",
    "layer_config = {\n",
    "    '1':1,\n",
    "    '2':2,\n",
    "    '3':3\n",
    "}\n",
    "\n",
    "ansatz_config = {\n",
    "        #'IQP': IQPAnsatz,\n",
    "        'Sim14': Sim14Ansatz,\n",
    "        'Sim15': Sim15Ansatz,\n",
    "    }\n",
    "\n",
    "loss_function_config = {\n",
    "        'mse': MSELoss(),\n",
    "    }\n",
    "\n",
    "optimizer_config = {\n",
    "        'spsa': SPSAOptimizer,\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "SEED = 42\n",
    "for ansatz_name, ansatz in reversed(ansatz_config.items()):\n",
    "    for ansatz_param_name, ansatz_param in reversed(ansatz_param_config.items()):\n",
    "        for layer_name, layer in reversed(layer_config.items()):\n",
    "            for loss_name, loss_function in reversed(loss_function_config.items()):\n",
    "                for optimizer_name, optimizer in reversed(optimizer_config.items()):\n",
    "                    for o_param_name, o_param in reversed(optimizer_param_config.items()):\n",
    "                        run_quantum_trainer('random', ansatz(ansatz_param, n_layers = layer), loss_function, optimizer, o_param, EPOCHS, BATCH_SIZE, SEED, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
