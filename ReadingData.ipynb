{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\Desktop\\MastersProject\\QC_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import transformers\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoding():\n",
    "\t#https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX#scrollTo=Zn0n2S-FWZih\n",
    "\t\"\"\"\n",
    "\tReads a CSV of format:\n",
    "\t\tword1 word2, word3 word4, label\n",
    "\tWhere word1&2 make sentence 1 and 3&4 make sentence2.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, csv_dir):\n",
    "\t\tself.data = self.listOfRowsFromSCV(csv_dir)\n",
    "\t\tself.model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True).eval()# output_hidden_states is whether the model returns all hidden-states. # Put the model in \"evaluation\" mode, meaning feed-forward operation. \n",
    "\t\tpass\n",
    "\n",
    "\tdef listOfRowsFromSCV(self, csv_dir):\n",
    "\t\trows = []\n",
    "\t\twith open(csv_dir, \"r\") as csvfile:\n",
    "\t\t\treader_variable = csv.reader(csvfile, delimiter=\",\")\n",
    "\t\t\tfor row in reader_variable:\n",
    "\t\t\t\trows.append(row)\n",
    "\t\treturn rows\t\n",
    "\t\n",
    "\tdef getSentEmbedding(self, sent):\n",
    "\t\tmarked_text = \"[CLS] \" + sent + \" [SEP]\"\n",
    "\t\ttokenized_text = tokenizer.tokenize(marked_text)\t# Tokenize our sentence with the BERT tokenizer.\n",
    "\t\treturn(tokenized_text)\t# Print out the tokens.\n",
    "\n",
    "\tdef preprocessSentence(self, sent, printing=None):\n",
    "\t\t# Add the special tokens.\n",
    "\t\tmarked_text = \"[CLS] \" + sent + \" [SEP]\"\n",
    "\t\t# Split the sentence into tokens.\n",
    "\t\ttokenized_text = tokenizer.tokenize(marked_text)\n",
    "\t\t# Map the token strings to their vocabulary indeces.\n",
    "\t\tindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\t\tif printing:\n",
    "\t\t\t# Display the words with their indeces.\n",
    "\t\t\tfor tup in zip(tokenized_text, indexed_tokens):\n",
    "\t\t\t\tprint('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "\t\t# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "\t\tsegments_ids = [1] * len(tokenized_text)\n",
    "\t\tif printing:\n",
    "\t\t\tprint(segments_ids)\n",
    "\n",
    "\t\t# Convert inputs to PyTorch tensors\n",
    "\t\ttokens_tensor = torch.tensor([indexed_tokens])\n",
    "\t\tsegments_tensors = torch.tensor([segments_ids])\n",
    "\t\treturn tokens_tensor, segments_tensors\n",
    "\t\n",
    "\tdef hiddenLayersBERT(self, sentences):\n",
    "\t\t# Run the text through BERT, and collect all of the hidden states produced from all 12 layers. \n",
    "\t\thidden_states = [] \n",
    "\t\tfor idx, sentence in enumerate(sentences):\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = self.model(sentence[0], sentence[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Evaluating the model will return a different number of objects based on \n",
    "\t\t\t\t# how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "\t\t\t\t# becase we set `output_hidden_states = True`, the third item will be the \n",
    "\t\t\t\t# hidden states from all layers. See the documentation for more details:\n",
    "\t\t\t\t# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\t\t\t\thidden_states.append(outputs[2])\n",
    "\t\treturn hidden_states\n",
    "\n",
    "\tdef GetTokenVecSum(self, hidden_state):\n",
    "\t\t# Concatenate the tensors for all layers. We use `stack` here to create a new dimension in the tensor.\n",
    "\t\t# Remove dimension 1, the \"batches\".\n",
    "\t\t# Swap dimensions 0 and 1.\n",
    "\t\ttoken_embeddings = torch.squeeze(torch.stack(hidden_state, dim=0), dim=1).permute(1,0,2)\n",
    "\n",
    "\t\t# `hidden_state` has shape [13 x 1 x 22 x 768]\n",
    "\t\t# `token_vecs` is a tensor with shape [22 x 768]\n",
    "\t\ttoken_vecs = hidden_state[-2][0]\n",
    "\n",
    "\t\t# Calculate the average of all 22 token vectors.\n",
    "\t\tsentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "\t\t#stores the token vectors, with shape [22 x 768]\n",
    "\t\ttoken_vecs_sum = []\n",
    "\n",
    "\t\t# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\t\tfor token in token_embeddings:\n",
    "\t\t\t# `token` is a [12 x 768] tensor\n",
    "\t\t\t# Sum the vectors from the last four layers.\n",
    "\t\t\tsum_vec = torch.sum(token[-4:], dim=0)\n",
    "\t\t\t# Use `sum_vec` to represent `token`.\n",
    "\t\t\ttoken_vecs_sum.append(sum_vec)\n",
    "\n",
    "\t\t#print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\t\treturn token_vecs_sum, sentence_embedding\n",
    "\n",
    "\tdef GetEmbeddingsForBothSentences(self, sentences):\n",
    "\t\ttoken_embedding_list = []\n",
    "\t\tsentence_embedding_list = []\n",
    "\t\tfor idx, sentence in enumerate(sentences):\n",
    "\t\t\ttoken_embedding_list.append(self.GetTokenVecSum(sentence)[0])\n",
    "\t\t\tsentence_embedding_list.append(self.GetTokenVecSum(sentence)[1])\n",
    "\t\treturn token_embedding_list, sentence_embedding_list\n",
    "\t\n",
    "\tdef CosineSimilarity(self, sent0, sent1):\n",
    "\t\t# Calculate the cosine similarity between the word bank in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "\t\treturn 1-cosine(sent0, sent1)\n",
    "\t\n",
    "\tdef evaluateSentPair(self, row_number):\n",
    "\t\tsents_to_evaluate = [self.preprocessSentence(self.data[row_number][0]), self.preprocessSentence(self.data[row_number][1])]\n",
    "\n",
    "\t\thidden = self.hiddenLayersBERT(sents_to_evaluate)\n",
    "\t\tembeddings = self.GetEmbeddingsForBothSentences(hidden)\n",
    "\n",
    "\t\treturn self.CosineSimilarity(embeddings[1][0], embeddings[1][1])\n",
    "\t\n",
    "\tdef evaluateAllSentPairs(self):\n",
    "\t\tres = []\n",
    "\t\tfor i in range(len(self.data)):\n",
    "\t\t\tres.append(self.evaluateSentPair(i))\n",
    "\t\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9686732292175293,\n",
       " 0.7988609075546265,\n",
       " 0.8739149570465088,\n",
       " 0.8251831531524658,\n",
       " 0.813016951084137,\n",
       " 0.8406080007553101,\n",
       " 0.8638482093811035,\n",
       " 0.9042112827301025,\n",
       " 0.8447734713554382,\n",
       " 0.6192495226860046,\n",
       " 0.6420484781265259,\n",
       " 0.6192495226860046,\n",
       " 0.5932442545890808,\n",
       " 0.7548890709877014,\n",
       " 0.8230564594268799,\n",
       " 0.7330701947212219,\n",
       " 0.817246675491333,\n",
       " 0.7026510238647461,\n",
       " 0.743051290512085,\n",
       " 0.8083339333534241,\n",
       " 0.7476651668548584,\n",
       " 0.8074460625648499,\n",
       " 0.7038283944129944,\n",
       " 0.7743031978607178,\n",
       " 0.7347809672355652,\n",
       " 0.6991939544677734,\n",
       " 0.6020883917808533,\n",
       " 0.7560901641845703,\n",
       " 0.6456645131111145,\n",
       " 0.8126022219657898,\n",
       " 0.6224542856216431,\n",
       " 0.609408974647522,\n",
       " 0.6590306758880615,\n",
       " 0.7127388119697571,\n",
       " 0.6719609498977661,\n",
       " 0.653028130531311,\n",
       " 0.6624095439910889]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.evaluateAllSentPairs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d189fd2b47ddadb241d7d165de12d7185a13a06e223542173d7933cb832f0e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
